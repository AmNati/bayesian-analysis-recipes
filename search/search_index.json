{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Bayesian Analysis Recipes Introduction I've recently been inspired by how flexible and powerful Bayesian statistical analysis can be. Yet, as with many things, flexibility often means a tradeoff with ease-of-use. I think having a cookbook of code that can be used in a number of settings can be extremely helpful for bringing Bayesian methods to a more general setting! Notebooks There is one notebook per model. In each notebook, you should end up finding: The kind of problem that is being tackled here. A description of how the data should be structured. An example data table. It generally will end up being tidy data. PyMC3 code for the model; in some notebooks, there may be two versions of the same model. Examples on how to report findings from the MCMC-sampled posterior. It is my hope that these recipes will be useful for you! (hypo)thesis My hypothesis here follows the Pareto principle: a large fraction of real-world problems can essentially be modelled with a core collection of models, each of which have a Bayesian interpretation. In particular, I have this hunch that commonly-used methods like ANOVA can be replaced by conceptually simpler and much more interpretable Bayesian alternatives, like John Kruschke's BEST ( B ayesian E stimation S upersedes the T -test). For example, ANOVA only tests whether means of multiple treatment groups are the same or not... but BEST gives us the estimated posterior distribution over each of the treatment groups, assuming each treatment group is i.i.d. Hence, richer information can be gleaned: we can, given the data at hand, make statements about how any particular pair of groups are different, without requiring additional steps such as multiple hypothesis corrections. Further reading/watching/listening Books: Bayesian Methods for Hackers Think Bayes Papers: Bayesian Estimation Supersedes the t-Test Videos: Computational Statistics I @ SciPy 2015 Computational Statistics II @ SciPy 2015 Bayesian Statistical Analysis with Python @ PyCon 2017 Bayesian Estimation Supersedes the t-Test Got Feedback? There's a few ways you can help make this repository an awesome one for Bayesian method learners out there. If you have a question: Post a GitHub issue with your question. I'll try my best to respond as soon as possible. If you have a suggested change: Submit a pull request detailing the change and why you think it's important. Keep it simple, no need to have essay-length justifications; this also makes things easier for me to review. As usual, I will get back to you as soon as I can.","title":"Bayesian Analysis Recipes"},{"location":"#bayesian-analysis-recipes","text":"","title":"Bayesian Analysis Recipes"},{"location":"#introduction","text":"I've recently been inspired by how flexible and powerful Bayesian statistical analysis can be. Yet, as with many things, flexibility often means a tradeoff with ease-of-use. I think having a cookbook of code that can be used in a number of settings can be extremely helpful for bringing Bayesian methods to a more general setting!","title":"Introduction"},{"location":"#notebooks","text":"There is one notebook per model. In each notebook, you should end up finding: The kind of problem that is being tackled here. A description of how the data should be structured. An example data table. It generally will end up being tidy data. PyMC3 code for the model; in some notebooks, there may be two versions of the same model. Examples on how to report findings from the MCMC-sampled posterior. It is my hope that these recipes will be useful for you!","title":"Notebooks"},{"location":"#hypothesis","text":"My hypothesis here follows the Pareto principle: a large fraction of real-world problems can essentially be modelled with a core collection of models, each of which have a Bayesian interpretation. In particular, I have this hunch that commonly-used methods like ANOVA can be replaced by conceptually simpler and much more interpretable Bayesian alternatives, like John Kruschke's BEST ( B ayesian E stimation S upersedes the T -test). For example, ANOVA only tests whether means of multiple treatment groups are the same or not... but BEST gives us the estimated posterior distribution over each of the treatment groups, assuming each treatment group is i.i.d. Hence, richer information can be gleaned: we can, given the data at hand, make statements about how any particular pair of groups are different, without requiring additional steps such as multiple hypothesis corrections.","title":"(hypo)thesis"},{"location":"#further-readingwatchinglistening","text":"Books: Bayesian Methods for Hackers Think Bayes Papers: Bayesian Estimation Supersedes the t-Test Videos: Computational Statistics I @ SciPy 2015 Computational Statistics II @ SciPy 2015 Bayesian Statistical Analysis with Python @ PyCon 2017 Bayesian Estimation Supersedes the t-Test","title":"Further reading/watching/listening"},{"location":"#got-feedback","text":"There's a few ways you can help make this repository an awesome one for Bayesian method learners out there. If you have a question: Post a GitHub issue with your question. I'll try my best to respond as soon as possible. If you have a suggested change: Submit a pull request detailing the change and why you think it's important. Keep it simple, no need to have essay-length justifications; this also makes things easier for me to review. As usual, I will get back to you as soon as I can.","title":"Got Feedback?"},{"location":"notebooks/bayesian-estimation-on-multiple-groups/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Bayesian Estimation on Multiple Groups Problem Type The Bayesian estimation model is widely applicable across a number of scenarios. The classical scenario is when we have an experimental design where there is a control vs. a treatment, and we want to know what the difference is between the two. Here, \"estimation\" is used to estimate the \"true\" value for the control and the \"true\" value for the treatment, and the \"Bayesian\" part refers to the computation of the uncertainty surrounding the parameter. Bayesian estimation's advantages over the classical t-test was first described by John Kruschke (2013). In this notebook, I provide a concise implementation suitable for two-sample and multi-sample inference, with data that don't necessarily fit Gaussian assumptions. Data structure To use it with this model, the data should be structured as such: Each row is one measurement. The columns should indicate, at the minimum: What treatment group the sample belonged to. The measured value. Extensions to the model As of now, the model only samples posterior distributions of measured values. The model, then, may be extended to compute differences in means (sample vs. control) or effect sizes, complete with uncertainty around it. Use pm.Deterministic(...) to ensure that those statistics' posterior distributions, i.e. uncertainty, are also computed. Reporting summarized findings Here are examples of how to summarize the findings. Treatment group A was greater than control by x units (95% HPD: [ lower , upper ]). Treatment group A was higher than control (effect size 95% HPD: [ lower , upper ]). % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' import pymc4 as pm4 import pandas as pd import numpy as np import matplotlib.pyplot as plt from matplotlib.gridspec import GridSpec import janitor as jn from utils import ecdf from pyprojroot import here # Read in the data df = ( pd . read_csv ( here () / \"datasets/biofilm.csv\" ) . label_encode ( columns = [ \"isolate\" ]) # encode isolate as labels. . transform_column ( \"normalized_measurement\" , np . log , \"log_normalized_measurement\" ) ) # Display a subset of the data. df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } experiment isolate ST OD600 measurement replicate normalized_measurement isolate_enc log_normalized_measurement 0 1 1 4 0.461 0.317 1 0.687636 0 -0.374496 1 1 2 55 0.346 0.434 1 1.254335 7 0.226606 2 1 3 55 0.356 0.917 1 2.575843 8 0.946177 3 1 4 4 0.603 1.061 1 1.759536 9 0.565050 4 1 5 330 0.444 3.701 1 8.335586 10 2.120534 Model Specification We know that the OD600 and measurements columns are all positive-valued, and so the normalized_measurement column will also be positive-valued. There are two ways to handle this situation: We can either choose to directly model the likelihood using a bounded, positive-support-only distribution, or We can model the log-transformation of the normalized_measurement column, using an unbounded, infinite-support distribution (e.g. the T-distribution family of distributions, which includes the Gaussian and the Cauchy in there). The former is ever slightly more convenient to reason about, but the latter lets us use Gaussians, which have some nice properties when sampling. import tensorflow as tf import numpy as np num_isolates = len ( set ( df [ \"isolate_enc\" ])) @pm4 . model def bacteria_model (): mu_mean = yield pm4 . Normal ( \"mu_mean\" , loc = 0 , scale = 1 ) mu = yield pm4 . Normal ( \"mu\" , loc = mu_mean , scale = 1 , batch_stack = num_isolates ) mu_bounded = yield pm4 . Deterministic ( \"mu_bounded\" , tf . exp ( mu )) # Because we use TFP, tf.gather now replaces the old numpy syntax. # the following line is equivalent to: # mu_all = mu[df[\"isolate_enc\"]] mu_all = tf . gather ( mu , df [ \"isolate_enc\" ]) sigma = yield pm4 . HalfCauchy ( \"sd\" , scale = 1 , batch_stack = num_isolates ) sigma_all = tf . gather ( sigma , df [ \"isolate_enc\" ]) nu = yield pm4 . Exponential ( \"nu\" , rate = 1 / 30. ) like = yield pm4 . StudentT ( \"like\" , loc = mu_all , scale = sigma_all , df = nu , observed = df [ \"log_normalized_measurement\" ]) # Take the difference against the ATCC strain, which is the control. difference = yield pm4 . Deterministic ( \"difference\" , mu_bounded [: - 1 ] - mu_bounded [ - 1 ]) Inference Button! Now, we hit the Inference Button(tm) and sample from the posterior distribution. trace = pm4 . sample ( bacteria_model ()) Diagnostics Our first diagnostic will be the trace plot. We expect the trace of the variables that we are most interested in to be a fuzzy caterpillar. import arviz as az axes = az . plot_trace ( trace , var_names = [ \"bacteria_model/mu\" ], compact = True ) Looking at the traces, yes, everything looks more or less like a hairy caterpillar. This means that sampling went well, and has converged, thus we have a good MCMC estimator of the posterior distribution. I need a mapping of isolate to its encoding - will come in handy below. mapping = dict ( zip ( df [ \"isolate_enc\" ], df [ \"isolate\" ])) yticklabels = list ( reversed ([ mapping [ i ] for i in range ( len ( mapping ))])) Let's now plot the posterior distributions. We'll use a ridge plot, as it's both aesthetically pleasing and informative. fig , ax = plt . subplots ( figsize = ( 8 , 6 )) axes = az . plot_forest ( trace , var_names = [ \"bacteria_model/mu_bounded\" ], ax = ax , kind = \"ridgeplot\" ) axes [ 0 ] . set_yticklabels ( yticklabels ); On the basis of this, we would say that strain 5 was the most different from the other strains. Let's now look at the differences directly. fig , ax = plt . subplots ( figsize = ( 8 , 6 )) axes = az . plot_forest ( trace , var_names = [ \"bacteria_model/difference\" ], ax = ax , kind = \"ridgeplot\" ) axes [ 0 ] . axvline ( 0 , color = \"black\" ) axes [ 0 ] . set_yticklabels ( yticklabels [ 1 :]); If we were in a binary decision-making mode, we would say that isolates 5 was the most \"significantly\" different from the ATCC strain. trace_with_posterior = pm4 . sample_posterior_predictive ( bacteria_model (), trace = trace ) WARNING:tensorflow:AutoGraph could not transform <function pfor.<locals>.f at 0x7f2ad001ddc0> and will run it as-is. Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Inconsistent ASTs detected. This is a bug. Cause: inconsistent values for field args: [<gast.gast.Name object at 0x7f2a107f0820>, <gast.gast.Name object at 0x7f2a107f05e0>, <gast.gast.Name object at 0x7f2a107f0460>] and []Diff: *** Original nodes --- Reparsed nodes *************** *** 41,55 **** | | | ] | | | value=Constant: | | | | value=None | | | | kind=None | | FunctionDef: | | | name=u\"create_converted_entity\" | | | args=arguments: ! | | | | args=[ | | | | | Name: | | | | | | id=u\"ag__\" | | | | | | ctx=Param() | | | | | | annotation=None | | | | | | type_comment=None | | | | | Name: | | | | | | id=u\"ag_source_map__\" --- 41,56 ---- | | | ] | | | value=Constant: | | | | value=None | | | | kind=None | | FunctionDef: | | | name=u\"create_converted_entity\" | | | args=arguments: ! | | | | args=[] ! | | | | posonlyargs=[ | | | | | Name: | | | | | | id=u\"ag__\" | | | | | | ctx=Param() | | | | | | annotation=None | | | | | | type_comment=None | | | | | Name: | | | | | | id=u\"ag_source_map__\" *************** *** 58,72 **** | | | | | | type_comment=None | | | | | Name: | | | | | | id=u\"ag_module__\" | | | | | | ctx=Param() | | | | | | annotation=None | | | | | | type_comment=None | | | | ] - | | | | posonlyargs=[] | | | | vararg=None | | | | kwonlyargs=[] | | | | kw_defaults=[] | | | | kwarg=None | | | | defaults=[] | | | body=[ | | | | FunctionDef: --- 59,72 ---- To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert WARNING: AutoGraph could not transform <function pfor.<locals>.f at 0x7f2ad001ddc0> and will run it as-is. Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Inconsistent ASTs detected. This is a bug. Cause: inconsistent values for field args: [<gast.gast.Name object at 0x7f2a107f0820>, <gast.gast.Name object at 0x7f2a107f05e0>, <gast.gast.Name object at 0x7f2a107f0460>] and []Diff: *** Original nodes --- Reparsed nodes *************** *** 41,55 **** | | | ] | | | value=Constant: | | | | value=None | | | | kind=None | | FunctionDef: | | | name=u\"create_converted_entity\" | | | args=arguments: ! | | | | args=[ | | | | | Name: | | | | | | id=u\"ag__\" | | | | | | ctx=Param() | | | | | | annotation=None | | | | | | type_comment=None | | | | | Name: | | | | | | id=u\"ag_source_map__\" --- 41,56 ---- | | | ] | | | value=Constant: | | | | value=None | | | | kind=None | | FunctionDef: | | | name=u\"create_converted_entity\" | | | args=arguments: ! | | | | args=[] ! | | | | posonlyargs=[ | | | | | Name: | | | | | | id=u\"ag__\" | | | | | | ctx=Param() | | | | | | annotation=None | | | | | | type_comment=None | | | | | Name: | | | | | | id=u\"ag_source_map__\" *************** *** 58,72 **** | | | | | | type_comment=None | | | | | Name: | | | | | | id=u\"ag_module__\" | | | | | | ctx=Param() | | | | | | annotation=None | | | | | | type_comment=None | | | | ] - | | | | posonlyargs=[] | | | | vararg=None | | | | kwonlyargs=[] | | | | kw_defaults=[] | | | | kwarg=None | | | | defaults=[] | | | body=[ | | | | FunctionDef: --- 59,72 ---- To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert WARNING:tensorflow:AutoGraph could not transform <function _convert_function_call.<locals>.f at 0x7f2ad0133820> and will run it as-is. Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Inconsistent ASTs detected. This is a bug. Cause: inconsistent values for field args: [<gast.gast.Name object at 0x7f2a7c102f10>] and []Diff: *** Original nodes --- Reparsed nodes *************** *** 30,44 **** | | | ] | | | value=Constant: | | | | value=None | | | | kind=None | | FunctionDef: | | | name=u\"create_converted_entity\" | | | args=arguments: ! | | | | args=[ | | | | | Name: | | | | | | id=u\"ag__\" | | | | | | ctx=Param() | | | | | | annotation=None | | | | | | type_comment=None | | | | | Name: | | | | | | id=u\"ag_source_map__\" --- 30,45 ---- | | | ] | | | value=Constant: | | | | value=None | | | | kind=None | | FunctionDef: | | | name=u\"create_converted_entity\" | | | args=arguments: ! | | | | args=[] ! | | | | posonlyargs=[ | | | | | Name: | | | | | | id=u\"ag__\" | | | | | | ctx=Param() | | | | | | annotation=None | | | | | | type_comment=None | | | | | Name: | | | | | | id=u\"ag_source_map__\" *************** *** 47,61 **** | | | | | | type_comment=None | | | | | Name: | | | | | | id=u\"ag_module__\" | | | | | | ctx=Param() | | | | | | annotation=None | | | | | | type_comment=None | | | | ] - | | | | posonlyargs=[] | | | | vararg=None | | | | kwonlyargs=[] | | | | kw_defaults=[] | | | | kwarg=None | | | | defaults=[] | | | body=[ | | | | FunctionDef: --- 48,61 ---- *************** *** 372,415 **** | | | | | | | | | ] | | | | | | | | | decorator_list=[] | | | | | | | | | returns=None | | | | | | | | | type_comment=None | | | | | | | | FunctionDef: | | | | | | | | | name=u\"set_state\" | | | | | | | | | args=arguments: ! | | | | | | | | | | args=[ | | | | | | | | | | | Name: | | | | | | | | | | | | id=u\"loop_vars\" | | | | | | | | | | | | ctx=Param() | | | | | | | | | | | | annotation=None | | | | | | | | | | | | type_comment=None | | | | | | | | | | ] - | | | | | | | | | | posonlyargs=[] | | | | | | | | | | vararg=None | | | | | | | | | | kwonlyargs=[] | | | | | | | | | | kw_defaults=[] | | | | | | | | | | kwarg=None | | | | | | | | | | defaults=[] | | | | | | | | | body=[ ! | | | | | | | | | | Pass() | | | | | | | | | ] | | | | | | | | | decorator_list=[] | | | | | | | | | returns=None | | | | | | | | | type_comment=None | | | | | | | | FunctionDef: | | | | | | | | | name=u\"loop_body\" | | | | | | | | | args=arguments: ! | | | | | | | | | | args=[ | | | | | | | | | | | Name: | | | | | | | | | | | | id=u\"itr\" | | | | | | | | | | | | ctx=Param() | | | | | | | | | | | | annotation=None | | | | | | | | | | | | type_comment=None | | | | | | | | | | ] - | | | | | | | | | | posonlyargs=[] | | | | | | | | | | vararg=None | | | | | | | | | | kwonlyargs=[] | | | | | | | | | | kw_defaults=[] | | | | | | | | | | kwarg=None | | | | | | | | | | defaults=[] | | | | | | | | | body=[ | | | | | | | | | | Assign: --- 372,415 ---- | | | | | | | | | ] | | | | | | | | | decorator_list=[] | | | | | | | | | returns=None | | | | | | | | | type_comment=None | | | | | | | | FunctionDef: | | | | | | | | | name=u\"set_state\" | | | | | | | | | args=arguments: ! | | | | | | | | | | args=[] ! | | | | | | | | | | posonlyargs=[ | | | | | | | | | | | Name: | | | | | | | | | | | | id=u\"loop_vars\" | | | | | | | | | | | | ctx=Param() | | | | | | | | | | | | annotation=None | | | | | | | | | | | | type_comment=None | | | | | | | | | | ] | | | | | | | | | | vararg=None | | | | | | | | | | kwonlyargs=[] | | | | | | | | | | kw_defaults=[] | | | | | | | | | | kwarg=None | | | | | | | | | | defaults=[] | | | | | | | | | body=[ ! | | | | | | | | | | Pass: | | | | | | | | | ] | | | | | | | | | decorator_list=[] | | | | | | | | | returns=None | | | | | | | | | type_comment=None | | | | | | | | FunctionDef: | | | | | | | | | name=u\"loop_body\" | | | | | | | | | args=arguments: ! | | | | | | | | | | args=[] ! | | | | | | | | | | posonlyargs=[ | | | | | | | | | | | Name: | | | | | | | | | | | | id=u\"itr\" | | | | | | | | | | | | ctx=Param() | | | | | | | | | | | | annotation=None | | | | | | | | | | | | type_comment=None | | | | | | | | | | ] | | | | | | | | | | vararg=None | | | | | | | | | | kwonlyargs=[] | | | | | | | | | | kw_defaults=[] | | | | | | | | | | kwarg=None | | | | | | | | | | defaults=[] | | | | | | | | | body=[ | | | | | | | | | | Assign: To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert WARNING: AutoGraph could not transform <function _convert_function_call.<locals>.f at 0x7f2ad0133820> and will run it as-is. Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Inconsistent ASTs detected. This is a bug. Cause: inconsistent values for field args: [<gast.gast.Name object at 0x7f2a7c102f10>] and []Diff: *** Original nodes --- Reparsed nodes *************** *** 30,44 **** | | | ] | | | value=Constant: | | | | value=None | | | | kind=None | | FunctionDef: | | | name=u\"create_converted_entity\" | | | args=arguments: ! | | | | args=[ | | | | | Name: | | | | | | id=u\"ag__\" | | | | | | ctx=Param() | | | | | | annotation=None | | | | | | type_comment=None | | | | | Name: | | | | | | id=u\"ag_source_map__\" --- 30,45 ---- | | | ] | | | value=Constant: | | | | value=None | | | | kind=None | | FunctionDef: | | | name=u\"create_converted_entity\" | | | args=arguments: ! | | | | args=[] ! | | | | posonlyargs=[ | | | | | Name: | | | | | | id=u\"ag__\" | | | | | | ctx=Param() | | | | | | annotation=None | | | | | | type_comment=None | | | | | Name: | | | | | | id=u\"ag_source_map__\" *************** *** 47,61 **** | | | | | | type_comment=None | | | | | Name: | | | | | | id=u\"ag_module__\" | | | | | | ctx=Param() | | | | | | annotation=None | | | | | | type_comment=None | | | | ] - | | | | posonlyargs=[] | | | | vararg=None | | | | kwonlyargs=[] | | | | kw_defaults=[] | | | | kwarg=None | | | | defaults=[] | | | body=[ | | | | FunctionDef: --- 48,61 ---- *************** *** 372,415 **** | | | | | | | | | ] | | | | | | | | | decorator_list=[] | | | | | | | | | returns=None | | | | | | | | | type_comment=None | | | | | | | | FunctionDef: | | | | | | | | | name=u\"set_state\" | | | | | | | | | args=arguments: ! | | | | | | | | | | args=[ | | | | | | | | | | | Name: | | | | | | | | | | | | id=u\"loop_vars\" | | | | | | | | | | | | ctx=Param() | | | | | | | | | | | | annotation=None | | | | | | | | | | | | type_comment=None | | | | | | | | | | ] - | | | | | | | | | | posonlyargs=[] | | | | | | | | | | vararg=None | | | | | | | | | | kwonlyargs=[] | | | | | | | | | | kw_defaults=[] | | | | | | | | | | kwarg=None | | | | | | | | | | defaults=[] | | | | | | | | | body=[ ! | | | | | | | | | | Pass() | | | | | | | | | ] | | | | | | | | | decorator_list=[] | | | | | | | | | returns=None | | | | | | | | | type_comment=None | | | | | | | | FunctionDef: | | | | | | | | | name=u\"loop_body\" | | | | | | | | | args=arguments: ! | | | | | | | | | | args=[ | | | | | | | | | | | Name: | | | | | | | | | | | | id=u\"itr\" | | | | | | | | | | | | ctx=Param() | | | | | | | | | | | | annotation=None | | | | | | | | | | | | type_comment=None | | | | | | | | | | ] - | | | | | | | | | | posonlyargs=[] | | | | | | | | | | vararg=None | | | | | | | | | | kwonlyargs=[] | | | | | | | | | | kw_defaults=[] | | | | | | | | | | kwarg=None | | | | | | | | | | defaults=[] | | | | | | | | | body=[ | | | | | | | | | | Assign: --- 372,415 ---- | | | | | | | | | ] | | | | | | | | | decorator_list=[] | | | | | | | | | returns=None | | | | | | | | | type_comment=None | | | | | | | | FunctionDef: | | | | | | | | | name=u\"set_state\" | | | | | | | | | args=arguments: ! | | | | | | | | | | args=[] ! | | | | | | | | | | posonlyargs=[ | | | | | | | | | | | Name: | | | | | | | | | | | | id=u\"loop_vars\" | | | | | | | | | | | | ctx=Param() | | | | | | | | | | | | annotation=None | | | | | | | | | | | | type_comment=None | | | | | | | | | | ] | | | | | | | | | | vararg=None | | | | | | | | | | kwonlyargs=[] | | | | | | | | | | kw_defaults=[] | | | | | | | | | | kwarg=None | | | | | | | | | | defaults=[] | | | | | | | | | body=[ ! | | | | | | | | | | Pass: | | | | | | | | | ] | | | | | | | | | decorator_list=[] | | | | | | | | | returns=None | | | | | | | | | type_comment=None | | | | | | | | FunctionDef: | | | | | | | | | name=u\"loop_body\" | | | | | | | | | args=arguments: ! | | | | | | | | | | args=[] ! | | | | | | | | | | posonlyargs=[ | | | | | | | | | | | Name: | | | | | | | | | | | | id=u\"itr\" | | | | | | | | | | | | ctx=Param() | | | | | | | | | | | | annotation=None | | | | | | | | | | | | type_comment=None | | | | | | | | | | ] | | | | | | | | | | vararg=None | | | | | | | | | | kwonlyargs=[] | | | | | | | | | | kw_defaults=[] | | | | | | | | | | kwarg=None | | | | | | | | | | defaults=[] | | | | | | | | | body=[ | | | | | | | | | | Assign: To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert WARNING:tensorflow:Note that RandomGamma inside pfor op may not give same output as inside a sequential loop. WARNING:tensorflow:Note that RandomStandardNormal inside pfor op may not give same output as inside a sequential loop. trace_with_posterior . posterior_predictive <xarray.Dataset> Dimensions: (bacteria_model/like_dim_0: 96, chain: 10, draw: 1000) Coordinates: * chain (chain) int64 0 1 2 3 4 5 6 7 8 9 * draw (draw) int64 0 1 2 3 4 5 ... 995 996 997 998 999 * bacteria_model/like_dim_0 (bacteria_model/like_dim_0) int64 0 1 2 ... 94 95 Data variables: bacteria_model/like (chain, draw, bacteria_model/like_dim_0) float32 -0.10798454 ... 0.7663585 Attributes: created_at: 2020-03-21T23:10:11.574949 arviz_version: 0.7.0 # We want indices for each of the samples. indices = dict () for enc , iso in mapping . items (): idxs = list ( df [ df [ \"isolate_enc\" ] == enc ] . index ) indices [ iso ] = idxs indices {'1': [0, 16, 32, 48, 64, 80], '2': [1, 17, 33, 49, 65, 81], '3': [2, 18, 34, 50, 66, 82], '4': [3, 19, 35, 51, 67, 83], '5': [4, 20, 36, 52, 68, 84], '6': [5, 21, 37, 53, 69, 85], '7': [6, 22, 38, 54, 70, 86], '8': [7, 23, 39, 55, 71, 87], '9': [8, 24, 40, 56, 72, 88], '10': [9, 25, 41, 57, 73, 89], '11': [10, 26, 42, 58, 74, 90], '12': [11, 27, 43, 59, 75, 91], '13': [12, 28, 44, 60, 76, 92], '14': [13, 29, 45, 61, 77, 93], '15': [14, 30, 46, 62, 78, 94], 'ATCC_29212': [15, 31, 47, 63, 79, 95]} # Make PPC plot for one of the groups. fig = plt . figure ( figsize = ( 16 , 16 )) gs = GridSpec ( nrows = 4 , ncols = 4 ) axes = dict () for i , ( strain , idxs ) in enumerate ( indices . items ()): if i > 0 : ax = fig . add_subplot ( gs [ i ], sharex = axes [ 0 ]) else : ax = fig . add_subplot ( gs [ i ]) x , y = ecdf ( df . iloc [ idxs ][ \"log_normalized_measurement\" ]) ax . plot ( x , y , label = \"data\" ) x , y = ecdf ( trace_with_posterior . posterior_predictive [ \"bacteria_model/like\" ] . loc [:, :, idxs ] . mean ( axis = ( 2 )) . data . flatten () ) ax . plot ( x , y , label = \"ppc\" ) ax . set_title ( f \"Strain { strain } \" ) axes [ i ] = ax The PPC draws clearly have longer tails than do the originals. I chalk this down to having small number of samples. The central tendency is definitely modelled well, and I don't see wild deviations between the sampled posterior and the measured data.","title":"Bayesian estimation on multiple groups"},{"location":"notebooks/bayesian-estimation-on-multiple-groups/#bayesian-estimation-on-multiple-groups","text":"","title":"Bayesian Estimation on Multiple Groups"},{"location":"notebooks/bayesian-estimation-on-multiple-groups/#problem-type","text":"The Bayesian estimation model is widely applicable across a number of scenarios. The classical scenario is when we have an experimental design where there is a control vs. a treatment, and we want to know what the difference is between the two. Here, \"estimation\" is used to estimate the \"true\" value for the control and the \"true\" value for the treatment, and the \"Bayesian\" part refers to the computation of the uncertainty surrounding the parameter. Bayesian estimation's advantages over the classical t-test was first described by John Kruschke (2013). In this notebook, I provide a concise implementation suitable for two-sample and multi-sample inference, with data that don't necessarily fit Gaussian assumptions.","title":"Problem Type"},{"location":"notebooks/bayesian-estimation-on-multiple-groups/#data-structure","text":"To use it with this model, the data should be structured as such: Each row is one measurement. The columns should indicate, at the minimum: What treatment group the sample belonged to. The measured value.","title":"Data structure"},{"location":"notebooks/bayesian-estimation-on-multiple-groups/#extensions-to-the-model","text":"As of now, the model only samples posterior distributions of measured values. The model, then, may be extended to compute differences in means (sample vs. control) or effect sizes, complete with uncertainty around it. Use pm.Deterministic(...) to ensure that those statistics' posterior distributions, i.e. uncertainty, are also computed.","title":"Extensions to the model"},{"location":"notebooks/bayesian-estimation-on-multiple-groups/#reporting-summarized-findings","text":"Here are examples of how to summarize the findings. Treatment group A was greater than control by x units (95% HPD: [ lower , upper ]). Treatment group A was higher than control (effect size 95% HPD: [ lower , upper ]). % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' import pymc4 as pm4 import pandas as pd import numpy as np import matplotlib.pyplot as plt from matplotlib.gridspec import GridSpec import janitor as jn from utils import ecdf from pyprojroot import here # Read in the data df = ( pd . read_csv ( here () / \"datasets/biofilm.csv\" ) . label_encode ( columns = [ \"isolate\" ]) # encode isolate as labels. . transform_column ( \"normalized_measurement\" , np . log , \"log_normalized_measurement\" ) ) # Display a subset of the data. df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } experiment isolate ST OD600 measurement replicate normalized_measurement isolate_enc log_normalized_measurement 0 1 1 4 0.461 0.317 1 0.687636 0 -0.374496 1 1 2 55 0.346 0.434 1 1.254335 7 0.226606 2 1 3 55 0.356 0.917 1 2.575843 8 0.946177 3 1 4 4 0.603 1.061 1 1.759536 9 0.565050 4 1 5 330 0.444 3.701 1 8.335586 10 2.120534","title":"Reporting summarized findings"},{"location":"notebooks/bayesian-estimation-on-multiple-groups/#model-specification","text":"We know that the OD600 and measurements columns are all positive-valued, and so the normalized_measurement column will also be positive-valued. There are two ways to handle this situation: We can either choose to directly model the likelihood using a bounded, positive-support-only distribution, or We can model the log-transformation of the normalized_measurement column, using an unbounded, infinite-support distribution (e.g. the T-distribution family of distributions, which includes the Gaussian and the Cauchy in there). The former is ever slightly more convenient to reason about, but the latter lets us use Gaussians, which have some nice properties when sampling. import tensorflow as tf import numpy as np num_isolates = len ( set ( df [ \"isolate_enc\" ])) @pm4 . model def bacteria_model (): mu_mean = yield pm4 . Normal ( \"mu_mean\" , loc = 0 , scale = 1 ) mu = yield pm4 . Normal ( \"mu\" , loc = mu_mean , scale = 1 , batch_stack = num_isolates ) mu_bounded = yield pm4 . Deterministic ( \"mu_bounded\" , tf . exp ( mu )) # Because we use TFP, tf.gather now replaces the old numpy syntax. # the following line is equivalent to: # mu_all = mu[df[\"isolate_enc\"]] mu_all = tf . gather ( mu , df [ \"isolate_enc\" ]) sigma = yield pm4 . HalfCauchy ( \"sd\" , scale = 1 , batch_stack = num_isolates ) sigma_all = tf . gather ( sigma , df [ \"isolate_enc\" ]) nu = yield pm4 . Exponential ( \"nu\" , rate = 1 / 30. ) like = yield pm4 . StudentT ( \"like\" , loc = mu_all , scale = sigma_all , df = nu , observed = df [ \"log_normalized_measurement\" ]) # Take the difference against the ATCC strain, which is the control. difference = yield pm4 . Deterministic ( \"difference\" , mu_bounded [: - 1 ] - mu_bounded [ - 1 ])","title":"Model Specification"},{"location":"notebooks/bayesian-estimation-on-multiple-groups/#inference-button","text":"Now, we hit the Inference Button(tm) and sample from the posterior distribution. trace = pm4 . sample ( bacteria_model ())","title":"Inference Button!"},{"location":"notebooks/bayesian-estimation-on-multiple-groups/#diagnostics","text":"Our first diagnostic will be the trace plot. We expect the trace of the variables that we are most interested in to be a fuzzy caterpillar. import arviz as az axes = az . plot_trace ( trace , var_names = [ \"bacteria_model/mu\" ], compact = True ) Looking at the traces, yes, everything looks more or less like a hairy caterpillar. This means that sampling went well, and has converged, thus we have a good MCMC estimator of the posterior distribution. I need a mapping of isolate to its encoding - will come in handy below. mapping = dict ( zip ( df [ \"isolate_enc\" ], df [ \"isolate\" ])) yticklabels = list ( reversed ([ mapping [ i ] for i in range ( len ( mapping ))])) Let's now plot the posterior distributions. We'll use a ridge plot, as it's both aesthetically pleasing and informative. fig , ax = plt . subplots ( figsize = ( 8 , 6 )) axes = az . plot_forest ( trace , var_names = [ \"bacteria_model/mu_bounded\" ], ax = ax , kind = \"ridgeplot\" ) axes [ 0 ] . set_yticklabels ( yticklabels ); On the basis of this, we would say that strain 5 was the most different from the other strains. Let's now look at the differences directly. fig , ax = plt . subplots ( figsize = ( 8 , 6 )) axes = az . plot_forest ( trace , var_names = [ \"bacteria_model/difference\" ], ax = ax , kind = \"ridgeplot\" ) axes [ 0 ] . axvline ( 0 , color = \"black\" ) axes [ 0 ] . set_yticklabels ( yticklabels [ 1 :]); If we were in a binary decision-making mode, we would say that isolates 5 was the most \"significantly\" different from the ATCC strain. trace_with_posterior = pm4 . sample_posterior_predictive ( bacteria_model (), trace = trace ) WARNING:tensorflow:AutoGraph could not transform <function pfor.<locals>.f at 0x7f2ad001ddc0> and will run it as-is. Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Inconsistent ASTs detected. This is a bug. Cause: inconsistent values for field args: [<gast.gast.Name object at 0x7f2a107f0820>, <gast.gast.Name object at 0x7f2a107f05e0>, <gast.gast.Name object at 0x7f2a107f0460>] and []Diff: *** Original nodes --- Reparsed nodes *************** *** 41,55 **** | | | ] | | | value=Constant: | | | | value=None | | | | kind=None | | FunctionDef: | | | name=u\"create_converted_entity\" | | | args=arguments: ! | | | | args=[ | | | | | Name: | | | | | | id=u\"ag__\" | | | | | | ctx=Param() | | | | | | annotation=None | | | | | | type_comment=None | | | | | Name: | | | | | | id=u\"ag_source_map__\" --- 41,56 ---- | | | ] | | | value=Constant: | | | | value=None | | | | kind=None | | FunctionDef: | | | name=u\"create_converted_entity\" | | | args=arguments: ! | | | | args=[] ! | | | | posonlyargs=[ | | | | | Name: | | | | | | id=u\"ag__\" | | | | | | ctx=Param() | | | | | | annotation=None | | | | | | type_comment=None | | | | | Name: | | | | | | id=u\"ag_source_map__\" *************** *** 58,72 **** | | | | | | type_comment=None | | | | | Name: | | | | | | id=u\"ag_module__\" | | | | | | ctx=Param() | | | | | | annotation=None | | | | | | type_comment=None | | | | ] - | | | | posonlyargs=[] | | | | vararg=None | | | | kwonlyargs=[] | | | | kw_defaults=[] | | | | kwarg=None | | | | defaults=[] | | | body=[ | | | | FunctionDef: --- 59,72 ---- To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert WARNING: AutoGraph could not transform <function pfor.<locals>.f at 0x7f2ad001ddc0> and will run it as-is. Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Inconsistent ASTs detected. This is a bug. Cause: inconsistent values for field args: [<gast.gast.Name object at 0x7f2a107f0820>, <gast.gast.Name object at 0x7f2a107f05e0>, <gast.gast.Name object at 0x7f2a107f0460>] and []Diff: *** Original nodes --- Reparsed nodes *************** *** 41,55 **** | | | ] | | | value=Constant: | | | | value=None | | | | kind=None | | FunctionDef: | | | name=u\"create_converted_entity\" | | | args=arguments: ! | | | | args=[ | | | | | Name: | | | | | | id=u\"ag__\" | | | | | | ctx=Param() | | | | | | annotation=None | | | | | | type_comment=None | | | | | Name: | | | | | | id=u\"ag_source_map__\" --- 41,56 ---- | | | ] | | | value=Constant: | | | | value=None | | | | kind=None | | FunctionDef: | | | name=u\"create_converted_entity\" | | | args=arguments: ! | | | | args=[] ! | | | | posonlyargs=[ | | | | | Name: | | | | | | id=u\"ag__\" | | | | | | ctx=Param() | | | | | | annotation=None | | | | | | type_comment=None | | | | | Name: | | | | | | id=u\"ag_source_map__\" *************** *** 58,72 **** | | | | | | type_comment=None | | | | | Name: | | | | | | id=u\"ag_module__\" | | | | | | ctx=Param() | | | | | | annotation=None | | | | | | type_comment=None | | | | ] - | | | | posonlyargs=[] | | | | vararg=None | | | | kwonlyargs=[] | | | | kw_defaults=[] | | | | kwarg=None | | | | defaults=[] | | | body=[ | | | | FunctionDef: --- 59,72 ---- To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert WARNING:tensorflow:AutoGraph could not transform <function _convert_function_call.<locals>.f at 0x7f2ad0133820> and will run it as-is. Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Inconsistent ASTs detected. This is a bug. Cause: inconsistent values for field args: [<gast.gast.Name object at 0x7f2a7c102f10>] and []Diff: *** Original nodes --- Reparsed nodes *************** *** 30,44 **** | | | ] | | | value=Constant: | | | | value=None | | | | kind=None | | FunctionDef: | | | name=u\"create_converted_entity\" | | | args=arguments: ! | | | | args=[ | | | | | Name: | | | | | | id=u\"ag__\" | | | | | | ctx=Param() | | | | | | annotation=None | | | | | | type_comment=None | | | | | Name: | | | | | | id=u\"ag_source_map__\" --- 30,45 ---- | | | ] | | | value=Constant: | | | | value=None | | | | kind=None | | FunctionDef: | | | name=u\"create_converted_entity\" | | | args=arguments: ! | | | | args=[] ! | | | | posonlyargs=[ | | | | | Name: | | | | | | id=u\"ag__\" | | | | | | ctx=Param() | | | | | | annotation=None | | | | | | type_comment=None | | | | | Name: | | | | | | id=u\"ag_source_map__\" *************** *** 47,61 **** | | | | | | type_comment=None | | | | | Name: | | | | | | id=u\"ag_module__\" | | | | | | ctx=Param() | | | | | | annotation=None | | | | | | type_comment=None | | | | ] - | | | | posonlyargs=[] | | | | vararg=None | | | | kwonlyargs=[] | | | | kw_defaults=[] | | | | kwarg=None | | | | defaults=[] | | | body=[ | | | | FunctionDef: --- 48,61 ---- *************** *** 372,415 **** | | | | | | | | | ] | | | | | | | | | decorator_list=[] | | | | | | | | | returns=None | | | | | | | | | type_comment=None | | | | | | | | FunctionDef: | | | | | | | | | name=u\"set_state\" | | | | | | | | | args=arguments: ! | | | | | | | | | | args=[ | | | | | | | | | | | Name: | | | | | | | | | | | | id=u\"loop_vars\" | | | | | | | | | | | | ctx=Param() | | | | | | | | | | | | annotation=None | | | | | | | | | | | | type_comment=None | | | | | | | | | | ] - | | | | | | | | | | posonlyargs=[] | | | | | | | | | | vararg=None | | | | | | | | | | kwonlyargs=[] | | | | | | | | | | kw_defaults=[] | | | | | | | | | | kwarg=None | | | | | | | | | | defaults=[] | | | | | | | | | body=[ ! | | | | | | | | | | Pass() | | | | | | | | | ] | | | | | | | | | decorator_list=[] | | | | | | | | | returns=None | | | | | | | | | type_comment=None | | | | | | | | FunctionDef: | | | | | | | | | name=u\"loop_body\" | | | | | | | | | args=arguments: ! | | | | | | | | | | args=[ | | | | | | | | | | | Name: | | | | | | | | | | | | id=u\"itr\" | | | | | | | | | | | | ctx=Param() | | | | | | | | | | | | annotation=None | | | | | | | | | | | | type_comment=None | | | | | | | | | | ] - | | | | | | | | | | posonlyargs=[] | | | | | | | | | | vararg=None | | | | | | | | | | kwonlyargs=[] | | | | | | | | | | kw_defaults=[] | | | | | | | | | | kwarg=None | | | | | | | | | | defaults=[] | | | | | | | | | body=[ | | | | | | | | | | Assign: --- 372,415 ---- | | | | | | | | | ] | | | | | | | | | decorator_list=[] | | | | | | | | | returns=None | | | | | | | | | type_comment=None | | | | | | | | FunctionDef: | | | | | | | | | name=u\"set_state\" | | | | | | | | | args=arguments: ! | | | | | | | | | | args=[] ! | | | | | | | | | | posonlyargs=[ | | | | | | | | | | | Name: | | | | | | | | | | | | id=u\"loop_vars\" | | | | | | | | | | | | ctx=Param() | | | | | | | | | | | | annotation=None | | | | | | | | | | | | type_comment=None | | | | | | | | | | ] | | | | | | | | | | vararg=None | | | | | | | | | | kwonlyargs=[] | | | | | | | | | | kw_defaults=[] | | | | | | | | | | kwarg=None | | | | | | | | | | defaults=[] | | | | | | | | | body=[ ! | | | | | | | | | | Pass: | | | | | | | | | ] | | | | | | | | | decorator_list=[] | | | | | | | | | returns=None | | | | | | | | | type_comment=None | | | | | | | | FunctionDef: | | | | | | | | | name=u\"loop_body\" | | | | | | | | | args=arguments: ! | | | | | | | | | | args=[] ! | | | | | | | | | | posonlyargs=[ | | | | | | | | | | | Name: | | | | | | | | | | | | id=u\"itr\" | | | | | | | | | | | | ctx=Param() | | | | | | | | | | | | annotation=None | | | | | | | | | | | | type_comment=None | | | | | | | | | | ] | | | | | | | | | | vararg=None | | | | | | | | | | kwonlyargs=[] | | | | | | | | | | kw_defaults=[] | | | | | | | | | | kwarg=None | | | | | | | | | | defaults=[] | | | | | | | | | body=[ | | | | | | | | | | Assign: To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert WARNING: AutoGraph could not transform <function _convert_function_call.<locals>.f at 0x7f2ad0133820> and will run it as-is. Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Inconsistent ASTs detected. This is a bug. Cause: inconsistent values for field args: [<gast.gast.Name object at 0x7f2a7c102f10>] and []Diff: *** Original nodes --- Reparsed nodes *************** *** 30,44 **** | | | ] | | | value=Constant: | | | | value=None | | | | kind=None | | FunctionDef: | | | name=u\"create_converted_entity\" | | | args=arguments: ! | | | | args=[ | | | | | Name: | | | | | | id=u\"ag__\" | | | | | | ctx=Param() | | | | | | annotation=None | | | | | | type_comment=None | | | | | Name: | | | | | | id=u\"ag_source_map__\" --- 30,45 ---- | | | ] | | | value=Constant: | | | | value=None | | | | kind=None | | FunctionDef: | | | name=u\"create_converted_entity\" | | | args=arguments: ! | | | | args=[] ! | | | | posonlyargs=[ | | | | | Name: | | | | | | id=u\"ag__\" | | | | | | ctx=Param() | | | | | | annotation=None | | | | | | type_comment=None | | | | | Name: | | | | | | id=u\"ag_source_map__\" *************** *** 47,61 **** | | | | | | type_comment=None | | | | | Name: | | | | | | id=u\"ag_module__\" | | | | | | ctx=Param() | | | | | | annotation=None | | | | | | type_comment=None | | | | ] - | | | | posonlyargs=[] | | | | vararg=None | | | | kwonlyargs=[] | | | | kw_defaults=[] | | | | kwarg=None | | | | defaults=[] | | | body=[ | | | | FunctionDef: --- 48,61 ---- *************** *** 372,415 **** | | | | | | | | | ] | | | | | | | | | decorator_list=[] | | | | | | | | | returns=None | | | | | | | | | type_comment=None | | | | | | | | FunctionDef: | | | | | | | | | name=u\"set_state\" | | | | | | | | | args=arguments: ! | | | | | | | | | | args=[ | | | | | | | | | | | Name: | | | | | | | | | | | | id=u\"loop_vars\" | | | | | | | | | | | | ctx=Param() | | | | | | | | | | | | annotation=None | | | | | | | | | | | | type_comment=None | | | | | | | | | | ] - | | | | | | | | | | posonlyargs=[] | | | | | | | | | | vararg=None | | | | | | | | | | kwonlyargs=[] | | | | | | | | | | kw_defaults=[] | | | | | | | | | | kwarg=None | | | | | | | | | | defaults=[] | | | | | | | | | body=[ ! | | | | | | | | | | Pass() | | | | | | | | | ] | | | | | | | | | decorator_list=[] | | | | | | | | | returns=None | | | | | | | | | type_comment=None | | | | | | | | FunctionDef: | | | | | | | | | name=u\"loop_body\" | | | | | | | | | args=arguments: ! | | | | | | | | | | args=[ | | | | | | | | | | | Name: | | | | | | | | | | | | id=u\"itr\" | | | | | | | | | | | | ctx=Param() | | | | | | | | | | | | annotation=None | | | | | | | | | | | | type_comment=None | | | | | | | | | | ] - | | | | | | | | | | posonlyargs=[] | | | | | | | | | | vararg=None | | | | | | | | | | kwonlyargs=[] | | | | | | | | | | kw_defaults=[] | | | | | | | | | | kwarg=None | | | | | | | | | | defaults=[] | | | | | | | | | body=[ | | | | | | | | | | Assign: --- 372,415 ---- | | | | | | | | | ] | | | | | | | | | decorator_list=[] | | | | | | | | | returns=None | | | | | | | | | type_comment=None | | | | | | | | FunctionDef: | | | | | | | | | name=u\"set_state\" | | | | | | | | | args=arguments: ! | | | | | | | | | | args=[] ! | | | | | | | | | | posonlyargs=[ | | | | | | | | | | | Name: | | | | | | | | | | | | id=u\"loop_vars\" | | | | | | | | | | | | ctx=Param() | | | | | | | | | | | | annotation=None | | | | | | | | | | | | type_comment=None | | | | | | | | | | ] | | | | | | | | | | vararg=None | | | | | | | | | | kwonlyargs=[] | | | | | | | | | | kw_defaults=[] | | | | | | | | | | kwarg=None | | | | | | | | | | defaults=[] | | | | | | | | | body=[ ! | | | | | | | | | | Pass: | | | | | | | | | ] | | | | | | | | | decorator_list=[] | | | | | | | | | returns=None | | | | | | | | | type_comment=None | | | | | | | | FunctionDef: | | | | | | | | | name=u\"loop_body\" | | | | | | | | | args=arguments: ! | | | | | | | | | | args=[] ! | | | | | | | | | | posonlyargs=[ | | | | | | | | | | | Name: | | | | | | | | | | | | id=u\"itr\" | | | | | | | | | | | | ctx=Param() | | | | | | | | | | | | annotation=None | | | | | | | | | | | | type_comment=None | | | | | | | | | | ] | | | | | | | | | | vararg=None | | | | | | | | | | kwonlyargs=[] | | | | | | | | | | kw_defaults=[] | | | | | | | | | | kwarg=None | | | | | | | | | | defaults=[] | | | | | | | | | body=[ | | | | | | | | | | Assign: To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert WARNING:tensorflow:Note that RandomGamma inside pfor op may not give same output as inside a sequential loop. WARNING:tensorflow:Note that RandomStandardNormal inside pfor op may not give same output as inside a sequential loop. trace_with_posterior . posterior_predictive <xarray.Dataset> Dimensions: (bacteria_model/like_dim_0: 96, chain: 10, draw: 1000) Coordinates: * chain (chain) int64 0 1 2 3 4 5 6 7 8 9 * draw (draw) int64 0 1 2 3 4 5 ... 995 996 997 998 999 * bacteria_model/like_dim_0 (bacteria_model/like_dim_0) int64 0 1 2 ... 94 95 Data variables: bacteria_model/like (chain, draw, bacteria_model/like_dim_0) float32 -0.10798454 ... 0.7663585 Attributes: created_at: 2020-03-21T23:10:11.574949 arviz_version: 0.7.0 # We want indices for each of the samples. indices = dict () for enc , iso in mapping . items (): idxs = list ( df [ df [ \"isolate_enc\" ] == enc ] . index ) indices [ iso ] = idxs indices {'1': [0, 16, 32, 48, 64, 80], '2': [1, 17, 33, 49, 65, 81], '3': [2, 18, 34, 50, 66, 82], '4': [3, 19, 35, 51, 67, 83], '5': [4, 20, 36, 52, 68, 84], '6': [5, 21, 37, 53, 69, 85], '7': [6, 22, 38, 54, 70, 86], '8': [7, 23, 39, 55, 71, 87], '9': [8, 24, 40, 56, 72, 88], '10': [9, 25, 41, 57, 73, 89], '11': [10, 26, 42, 58, 74, 90], '12': [11, 27, 43, 59, 75, 91], '13': [12, 28, 44, 60, 76, 92], '14': [13, 29, 45, 61, 77, 93], '15': [14, 30, 46, 62, 78, 94], 'ATCC_29212': [15, 31, 47, 63, 79, 95]} # Make PPC plot for one of the groups. fig = plt . figure ( figsize = ( 16 , 16 )) gs = GridSpec ( nrows = 4 , ncols = 4 ) axes = dict () for i , ( strain , idxs ) in enumerate ( indices . items ()): if i > 0 : ax = fig . add_subplot ( gs [ i ], sharex = axes [ 0 ]) else : ax = fig . add_subplot ( gs [ i ]) x , y = ecdf ( df . iloc [ idxs ][ \"log_normalized_measurement\" ]) ax . plot ( x , y , label = \"data\" ) x , y = ecdf ( trace_with_posterior . posterior_predictive [ \"bacteria_model/like\" ] . loc [:, :, idxs ] . mean ( axis = ( 2 )) . data . flatten () ) ax . plot ( x , y , label = \"ppc\" ) ax . set_title ( f \"Strain { strain } \" ) axes [ i ] = ax The PPC draws clearly have longer tails than do the originals. I chalk this down to having small number of samples. The central tendency is definitely modelled well, and I don't see wild deviations between the sampled posterior and the measured data.","title":"Diagnostics"},{"location":"notebooks/degrees-of-freedom/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Purpose Just testing my intuition w.r.t. degrees of freedom in the students T distribution. Cauchy: df = 1. Normal: df = infinity (or at least some really high number) This should be reflected when using PyMC3. import pymc3 as pm import numpy as np import matplotlib.pyplot as plt import arviz as az % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload normal = np . random . normal ( size = 20000 ) cauchy = np . random . standard_cauchy ( size = 20000 ) with pm . Model () as normal_model : mu = pm . Normal ( \"mu\" , mu = 0 , sd = 100 ) sd = pm . HalfNormal ( \"sd\" , sd = 100 ) nu = pm . Exponential ( \"nu\" , lam = 0.5 ) like = pm . StudentT ( \"like\" , mu = mu , sd = sd , nu = nu , observed = normal ) trace = pm . sample ( 2000 , tune = 2000 ) Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [nu, sd, mu] Sampling 4 chains, 0 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 16000/16000 [00:20<00:00, 790.93draws/s] axes = az . plot_trace ( trace ) Many degrees of freedom for normal distribution. Makes sense. with pm . Model () as cauchy_model : mu = pm . Normal ( \"mu\" , mu = 0 , sd = 100 ) sd = pm . HalfNormal ( \"sd\" , sd = 100 ) nu = pm . Exponential ( \"nu\" , lam = 1 ) like = pm . StudentT ( \"like\" , mu = mu , sd = sd , nu = nu , observed = cauchy ) trace = pm . sample ( 2000 , tune = 2000 ) Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [nu, sd, mu] Sampling 4 chains, 0 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 16000/16000 [00:22<00:00, 709.93draws/s] axes = az . plot_trace ( trace ) Basically 1 degree of freedom when inferring \\nu \\nu from Cauchy-distributed data. Yes :).","title":"Degrees of freedom"},{"location":"notebooks/degrees-of-freedom/#purpose","text":"Just testing my intuition w.r.t. degrees of freedom in the students T distribution. Cauchy: df = 1. Normal: df = infinity (or at least some really high number) This should be reflected when using PyMC3. import pymc3 as pm import numpy as np import matplotlib.pyplot as plt import arviz as az % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload normal = np . random . normal ( size = 20000 ) cauchy = np . random . standard_cauchy ( size = 20000 ) with pm . Model () as normal_model : mu = pm . Normal ( \"mu\" , mu = 0 , sd = 100 ) sd = pm . HalfNormal ( \"sd\" , sd = 100 ) nu = pm . Exponential ( \"nu\" , lam = 0.5 ) like = pm . StudentT ( \"like\" , mu = mu , sd = sd , nu = nu , observed = normal ) trace = pm . sample ( 2000 , tune = 2000 ) Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [nu, sd, mu] Sampling 4 chains, 0 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 16000/16000 [00:20<00:00, 790.93draws/s] axes = az . plot_trace ( trace ) Many degrees of freedom for normal distribution. Makes sense. with pm . Model () as cauchy_model : mu = pm . Normal ( \"mu\" , mu = 0 , sd = 100 ) sd = pm . HalfNormal ( \"sd\" , sd = 100 ) nu = pm . Exponential ( \"nu\" , lam = 1 ) like = pm . StudentT ( \"like\" , mu = mu , sd = sd , nu = nu , observed = cauchy ) trace = pm . sample ( 2000 , tune = 2000 ) Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [nu, sd, mu] Sampling 4 chains, 0 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 16000/16000 [00:22<00:00, 709.93draws/s] axes = az . plot_trace ( trace ) Basically 1 degree of freedom when inferring \\nu \\nu from Cauchy-distributed data. Yes :).","title":"Purpose"},{"location":"notebooks/dirichlet-multinomial-bayesian-proportions/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Introduction Let's say there are three bacteria species that characterize the gut, and we hypothesize that they are ever so shifted off from one another, but we don't know how (i.e. ignore the data-generating distribution below). Can we figure out the proportion parameters and their uncertainty? Generate Synthetic Data In the synthetic dataset generated below, we pretend that every patient is one sample, and we are recording the number of sequencing reads corresponding to some OTUs (bacteria). Each row is one sample (patient), and each column is one OTU (sample). Proportions Firstly, let's generate the ground truth proportions that we will infer later on. import numpy as np import pandas as pd import matplotlib.pyplot as plt import pymc3 as pm import numpy.random as npr % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' WARNING (theano.configdefaults): install mkl with `conda install mkl-service`: No module named 'mkl' def proportion ( arr ): arr = np . asarray ( arr ) return arr / arr . sum () healthy_proportions = proportion ([ 10 , 16 , 2 ]) healthy_proportions array([0.35714286, 0.57142857, 0.07142857]) sick_proportions = proportion ([ 10 , 27 , 15 ]) sick_proportions array([0.19230769, 0.51923077, 0.28846154]) Data Now, given the proportions, let's generate data. Here, we are assuming that there are 10 patients per cohort (10 sick patients and 10 healthy patients), and that the number of counts in total is 50. n_data_points = 10 def make_healthy_multinomial ( arr ): n_sequencing_reads = 50 # npr.poisson(lam=50) return npr . multinomial ( n_sequencing_reads , healthy_proportions ) def make_sick_multinomial ( arr ): n_sequencing_reads = 50 # npr.poisson(lam=50) return npr . multinomial ( n_sequencing_reads , sick_proportions ) # Generate healthy data healthy_reads = np . zeros (( n_data_points , 3 )) healthy_reads = np . apply_along_axis ( make_healthy_multinomial , axis = 1 , arr = healthy_reads ) # Generate sick reads sick_reads = np . zeros (( n_data_points , 3 )) sick_reads = np . apply_along_axis ( make_sick_multinomial , axis = 1 , arr = sick_reads ) # Make pandas dataframe healthy_df = pd . DataFrame ( healthy_reads ) healthy_df . columns = [ \"bacteria1\" , \"bacteria2\" , \"bacteria3\" ] healthy_df = pm . floatX ( healthy_df ) sick_df = pd . DataFrame ( sick_reads ) sick_df . columns = [ \"bacteria1\" , \"bacteria2\" , \"bacteria3\" ] sick_df = pm . floatX ( sick_df ) healthy_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bacteria1 bacteria2 bacteria3 0 18.0 28.0 4.0 1 16.0 33.0 1.0 2 19.0 30.0 1.0 3 15.0 31.0 4.0 4 17.0 29.0 4.0 5 16.0 26.0 8.0 6 19.0 29.0 2.0 7 16.0 27.0 7.0 8 21.0 25.0 4.0 9 21.0 27.0 2.0 sick_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bacteria1 bacteria2 bacteria3 0 3.0 29.0 18.0 1 13.0 20.0 17.0 2 11.0 25.0 14.0 3 7.0 27.0 16.0 4 8.0 29.0 13.0 5 11.0 23.0 16.0 6 5.0 28.0 17.0 7 7.0 25.0 18.0 8 13.0 20.0 17.0 9 9.0 25.0 16.0 Model Construction Here's an implementation of the model - Dirichlet prior with Multinomial likelihood. There are 3 classes of bacteria, so the Dirichlet distribution serves as the prior probability mass over each of the classes in the multinomial distribution. The multinomial distribution serves as the likelihood function. with pm . Model () as dirichlet_model : proportions_healthy = pm . Dirichlet ( \"proportions_healthy\" , a = np . array ([ 1.0 ] * 3 ) . astype ( \"float32\" ), shape = ( 3 ,), testval = [ 0.1 , 0.1 , 0.1 ], ) proportions_sick = pm . Dirichlet ( \"proportions_sick\" , a = np . array ([ 1.0 ] * 3 ) . astype ( \"float32\" ), shape = ( 3 ,), testval = [ 0.1 , 0.1 , 0.1 ], ) healthy_like = pm . Multinomial ( \"like_healthy\" , n = 50 , p = proportions_healthy , observed = healthy_df . values ) sick_like = pm . Multinomial ( \"like_sick\" , n = 50 , p = proportions_sick , observed = sick_df . values ) Sampling import arviz as az with dirichlet_model : dirichlet_trace = pm . sample ( 2000 ) az . plot_trace ( dirichlet_trace ) Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [proportions_sick, proportions_healthy] Sampling 4 chains, 0 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10000/10000 [00:03<00:00, 3301.52draws/s] Results ylabels = [ \"healthy_bacteria1\" , \"healthy_bacteria2\" , \"healthy_bacteria3\" , \"sick_bacteria1\" , \"sick_bacteria2\" , \"sick_bacteria3\" , ] axes = az . plot_forest ( dirichlet_trace ) healthy_proportions , sick_proportions (array([0.35714286, 0.57142857, 0.07142857]), array([0.19230769, 0.51923077, 0.28846154])) They match up with the original synthetic percentages!","title":"Dirichlet multinomial bayesian proportions"},{"location":"notebooks/dirichlet-multinomial-bayesian-proportions/#introduction","text":"Let's say there are three bacteria species that characterize the gut, and we hypothesize that they are ever so shifted off from one another, but we don't know how (i.e. ignore the data-generating distribution below). Can we figure out the proportion parameters and their uncertainty?","title":"Introduction"},{"location":"notebooks/dirichlet-multinomial-bayesian-proportions/#generate-synthetic-data","text":"In the synthetic dataset generated below, we pretend that every patient is one sample, and we are recording the number of sequencing reads corresponding to some OTUs (bacteria). Each row is one sample (patient), and each column is one OTU (sample).","title":"Generate Synthetic Data"},{"location":"notebooks/dirichlet-multinomial-bayesian-proportions/#proportions","text":"Firstly, let's generate the ground truth proportions that we will infer later on. import numpy as np import pandas as pd import matplotlib.pyplot as plt import pymc3 as pm import numpy.random as npr % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' WARNING (theano.configdefaults): install mkl with `conda install mkl-service`: No module named 'mkl' def proportion ( arr ): arr = np . asarray ( arr ) return arr / arr . sum () healthy_proportions = proportion ([ 10 , 16 , 2 ]) healthy_proportions array([0.35714286, 0.57142857, 0.07142857]) sick_proportions = proportion ([ 10 , 27 , 15 ]) sick_proportions array([0.19230769, 0.51923077, 0.28846154])","title":"Proportions"},{"location":"notebooks/dirichlet-multinomial-bayesian-proportions/#data","text":"Now, given the proportions, let's generate data. Here, we are assuming that there are 10 patients per cohort (10 sick patients and 10 healthy patients), and that the number of counts in total is 50. n_data_points = 10 def make_healthy_multinomial ( arr ): n_sequencing_reads = 50 # npr.poisson(lam=50) return npr . multinomial ( n_sequencing_reads , healthy_proportions ) def make_sick_multinomial ( arr ): n_sequencing_reads = 50 # npr.poisson(lam=50) return npr . multinomial ( n_sequencing_reads , sick_proportions ) # Generate healthy data healthy_reads = np . zeros (( n_data_points , 3 )) healthy_reads = np . apply_along_axis ( make_healthy_multinomial , axis = 1 , arr = healthy_reads ) # Generate sick reads sick_reads = np . zeros (( n_data_points , 3 )) sick_reads = np . apply_along_axis ( make_sick_multinomial , axis = 1 , arr = sick_reads ) # Make pandas dataframe healthy_df = pd . DataFrame ( healthy_reads ) healthy_df . columns = [ \"bacteria1\" , \"bacteria2\" , \"bacteria3\" ] healthy_df = pm . floatX ( healthy_df ) sick_df = pd . DataFrame ( sick_reads ) sick_df . columns = [ \"bacteria1\" , \"bacteria2\" , \"bacteria3\" ] sick_df = pm . floatX ( sick_df ) healthy_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bacteria1 bacteria2 bacteria3 0 18.0 28.0 4.0 1 16.0 33.0 1.0 2 19.0 30.0 1.0 3 15.0 31.0 4.0 4 17.0 29.0 4.0 5 16.0 26.0 8.0 6 19.0 29.0 2.0 7 16.0 27.0 7.0 8 21.0 25.0 4.0 9 21.0 27.0 2.0 sick_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bacteria1 bacteria2 bacteria3 0 3.0 29.0 18.0 1 13.0 20.0 17.0 2 11.0 25.0 14.0 3 7.0 27.0 16.0 4 8.0 29.0 13.0 5 11.0 23.0 16.0 6 5.0 28.0 17.0 7 7.0 25.0 18.0 8 13.0 20.0 17.0 9 9.0 25.0 16.0","title":"Data"},{"location":"notebooks/dirichlet-multinomial-bayesian-proportions/#model-construction","text":"Here's an implementation of the model - Dirichlet prior with Multinomial likelihood. There are 3 classes of bacteria, so the Dirichlet distribution serves as the prior probability mass over each of the classes in the multinomial distribution. The multinomial distribution serves as the likelihood function. with pm . Model () as dirichlet_model : proportions_healthy = pm . Dirichlet ( \"proportions_healthy\" , a = np . array ([ 1.0 ] * 3 ) . astype ( \"float32\" ), shape = ( 3 ,), testval = [ 0.1 , 0.1 , 0.1 ], ) proportions_sick = pm . Dirichlet ( \"proportions_sick\" , a = np . array ([ 1.0 ] * 3 ) . astype ( \"float32\" ), shape = ( 3 ,), testval = [ 0.1 , 0.1 , 0.1 ], ) healthy_like = pm . Multinomial ( \"like_healthy\" , n = 50 , p = proportions_healthy , observed = healthy_df . values ) sick_like = pm . Multinomial ( \"like_sick\" , n = 50 , p = proportions_sick , observed = sick_df . values )","title":"Model Construction"},{"location":"notebooks/dirichlet-multinomial-bayesian-proportions/#sampling","text":"import arviz as az with dirichlet_model : dirichlet_trace = pm . sample ( 2000 ) az . plot_trace ( dirichlet_trace ) Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [proportions_sick, proportions_healthy] Sampling 4 chains, 0 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10000/10000 [00:03<00:00, 3301.52draws/s]","title":"Sampling"},{"location":"notebooks/dirichlet-multinomial-bayesian-proportions/#results","text":"ylabels = [ \"healthy_bacteria1\" , \"healthy_bacteria2\" , \"healthy_bacteria3\" , \"sick_bacteria1\" , \"sick_bacteria2\" , \"sick_bacteria3\" , ] axes = az . plot_forest ( dirichlet_trace ) healthy_proportions , sick_proportions (array([0.35714286, 0.57142857, 0.07142857]), array([0.19230769, 0.51923077, 0.28846154])) They match up with the original synthetic percentages!","title":"Results"},{"location":"notebooks/hierarchical-baseball/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Introduction This notebook shows how to do hierarchical modelling with Binomially-distributed random variables. Problem Setup Baseball players have many metrics measured for them. Let's say we are on a baseball team, and would like to quantify player performance, one metric being their batting average (defined by how many times a batter hit a pitched ball, divided by the number of times they were up for batting (\"at bat\")). How would you go about this task? We first need some measurements of batting data. To answer this question, we need to have data on the number of time a player has batted and the number of times the player has hit the ball while batting. Let's see an example dataset below. % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' import pandas as pd import pymc3 as pm import matplotlib.pyplot as plt import numpy as np import theano.tensor as tt from pyprojroot import here WARNING (theano.configdefaults): install mkl with `conda install mkl-service`: No module named 'mkl' df = pd . read_csv ( here () / \"datasets/baseballdb/core/Batting.csv\" ) df [ \"AB\" ] = df [ \"AB\" ] . replace ( 0 , np . nan ) df = df . dropna () df [ \"batting_avg\" ] = df [ \"H\" ] / df [ \"AB\" ] df = df [ df [ \"yearID\" ] >= 2016 ] df = df . iloc [ 0 : 15 ] # select out only the first 15 players, just for illustration purposes. df . head ( 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } playerID yearID stint teamID lgID G AB R H 2B ... SB CS BB SO IBB HBP SH SF GIDP batting_avg 101333 abadfe01 2016 1 MIN AL 39 1.0 0 0 0 ... 0.0 0.0 0 1.0 0.0 0.0 0.0 0.0 0.0 0.000000 101335 abreujo02 2016 1 CHA AL 159 624.0 67 183 32 ... 0.0 2.0 47 125.0 7.0 15.0 0.0 9.0 21.0 0.293269 101337 ackledu01 2016 1 NYA AL 28 61.0 6 9 0 ... 0.0 0.0 8 9.0 0.0 0.0 0.0 1.0 0.0 0.147541 101338 adamecr01 2016 1 COL NL 121 225.0 25 49 7 ... 2.0 3.0 24 47.0 0.0 4.0 3.0 0.0 5.0 0.217778 101340 adamsma01 2016 1 SLN NL 118 297.0 37 74 18 ... 0.0 1.0 25 81.0 1.0 2.0 0.0 3.0 5.0 0.249158 5 rows \u00d7 23 columns In this dataset, the columns AB and H are the most relevent. AB is the number of times a player was A t B at. H is the number of times a player h it the ball while batting. The performance of a player can be defined by their batting percentage - essentially the number of hits divided by the number of times at bat. (Technically, a percentage should run from 0-100, but American sportspeople are apparently not very strict with how they approach these definitions.) Model 1: Naive Model One model that we can write is a model that assumes that each player has a batting percentage that is independent of the other players in the dataset. A pictorial view of the model is as such: Let's implement this model in PyMC3. with pm . Model () as baseline_model : thetas = pm . Beta ( \"thetas\" , alpha = 0.5 , beta = 0.5 , shape = ( len ( df ))) like = pm . Binomial ( \"likelihood\" , n = df [ \"AB\" ], p = thetas , observed = df [ \"H\" ]) with baseline_model : baseline_trace = pm . sample ( 2000 ) Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [thetas] Sampling 4 chains, 16 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10000/10000 [00:03<00:00, 2887.97draws/s] There were 5 divergences after tuning. Increase `target_accept` or reparameterize. There were 2 divergences after tuning. Increase `target_accept` or reparameterize. There were 8 divergences after tuning. Increase `target_accept` or reparameterize. There was 1 divergence after tuning. Increase `target_accept` or reparameterize. Let's view the posterior distribution traces. import arviz as az traceplot = az . plot_trace ( baseline_trace ) Looks like convergence has been achieved. From a Beta(\\alpha=0.5, \\beta=0.5) Beta(\\alpha=0.5, \\beta=0.5) prior, those players for which we have only 1 data point have very wide posterior distribution estimates. axes = az . plot_forest ( baseline_trace ) One of the big problems that we may have observed above is that the posterior distribution estimates for some players look very \"absurd\", raising a number of questions. For example: Do we really expect the 1 bat, 1 hit player to have such a high estimated batting average? Do we really expect the 1 bat, 0 hits player to have such a low estimated batting average? Don't we usually expect human performance to be approximately symmetrically distributed around some population mean? Model 2: Hierarchical Model With a hierarchical model, we can encode a set of assumptions into the model that may help us address the above questions. By a hierarchical model, we mean that each group's key parameter (in this case, the {\\theta} {\\theta} ) have a parental distribution placed on top of them. This is sometimes anthropomorphically called \"sharing information\" between the distributions. (Not saying that this is a good or bad thing, just naming it as it is.) The model diagram looks something like this: Let's start by specifying the model. with pm . Model () as baseball_model : phi = pm . Uniform ( \"phi\" , lower = 0.0 , upper = 1.0 ) kappa_log = pm . Exponential ( \"kappa_log\" , lam = 1.5 ) kappa = pm . Deterministic ( \"kappa\" , tt . exp ( kappa_log )) thetas = pm . Beta ( \"thetas\" , alpha = phi * kappa , beta = ( 1.0 - phi ) * kappa , shape = len ( df ) ) like = pm . Binomial ( \"like\" , n = df [ \"AB\" ], p = thetas , observed = df [ \"H\" ]) Then we sample from the posterior. with baseball_model : trace = pm . sample ( 2000 ) Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [thetas, kappa_log, phi] Sampling 4 chains, 23 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10000/10000 [00:06<00:00, 1448.88draws/s] There were 23 divergences after tuning. Increase `target_accept` or reparameterize. The number of effective samples is smaller than 10% for some parameters. Finally, let's inspect the traces for convergence. axes = az . plot_trace ( trace ) Convergence looks good! Let's also look at the posterior distribution of batting averages per player. ylabels = \"AB: \" + df [ \"AB\" ] . astype ( str ) + \", H: \" + df [ \"H\" ] . astype ( \"str\" ) ylabels = list ( reversed ( ylabels . tolist ())) axes = az . plot_forest ( trace , var_names = [ \"thetas\" ]) axes [ 0 ] . set_yticklabels ( ylabels ); It appears to me that these estimates are going to be much more reasonable. No player has a wildly high or low estimate on the basis of a single (or very few) data points. With a hierarchical model, we make the assumption that our observations (or treatments that group our observations) are somehow related. Under this assumption, when we have a new sample for which we have very few observations, we are able to borrow power from the population to make inferences about the new sample. Depending on the scenario, this assumption can either be reasonable, thereby not necessitating much debate, or be considered a \"strong assumption\", thereby requiring strong justification. Shrinkage \"Shrinkage\" is a term used to describe how hierarchical model estimation will usually result in parameter estimates that are \"shrunk\" away from their maximum likelihood estimators (i.e. the naive estimate from the data) towards the global mean. Shrinkage in and of itself is not necessarily a good or bad thing. However, because hierarchical models can sometimes be tricky to get right, we can use a shrinkage plot as a visual diagnostic for whether we have implemented the model correctly. # MLE per player mle = df [ \"batting_avg\" ] . values # Non-hierarchical model no_pool = baseline_trace [ \"thetas\" ] . mean ( axis = 0 ) # Hierarchical model partial_pool = trace [ \"thetas\" ] . mean ( axis = 0 ) # MLE over all players complete_pool = np . array ([ df [ \"batting_avg\" ] . mean ()] * len ( df )) fig = plt . figure () ax = fig . add_subplot ( 111 ) for r in np . vstack ([ mle , no_pool , partial_pool , complete_pool ]) . T : ax . plot ( r ) ax . set_xticks ([ 0 , 1 , 2 , 3 ]) ax . set_xticklabels ( [ \"maximum \\n likelihood\" , \"no \\n pooling\" , \"partial \\n pooling\" , \"complete \\n pooling\" ] ) ax . set_ylim ( 0 , 1 ) (0.0, 1.0) In this shrinkage plot, as we go from the maximum likelihood estimator (MLE) (essentially not explicitly specifying priors) to a no-pooling model (with weak priors), there is \"shrinkage\" of the estimates towards the population mean (complete pooling). Incorporating a parental prior on each group's parameters further constrains the credible range of parameters. This, then, is the phenomenon of \"shrinkage\" in modelling. Just to reiterate again -- there is nothing ineherently right or wrong about shrinkage. Whether this is reasonable or not depends on our prior information about the problem.","title":"Hierarchical baseball"},{"location":"notebooks/hierarchical-baseball/#introduction","text":"This notebook shows how to do hierarchical modelling with Binomially-distributed random variables.","title":"Introduction"},{"location":"notebooks/hierarchical-baseball/#problem-setup","text":"Baseball players have many metrics measured for them. Let's say we are on a baseball team, and would like to quantify player performance, one metric being their batting average (defined by how many times a batter hit a pitched ball, divided by the number of times they were up for batting (\"at bat\")). How would you go about this task? We first need some measurements of batting data. To answer this question, we need to have data on the number of time a player has batted and the number of times the player has hit the ball while batting. Let's see an example dataset below. % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' import pandas as pd import pymc3 as pm import matplotlib.pyplot as plt import numpy as np import theano.tensor as tt from pyprojroot import here WARNING (theano.configdefaults): install mkl with `conda install mkl-service`: No module named 'mkl' df = pd . read_csv ( here () / \"datasets/baseballdb/core/Batting.csv\" ) df [ \"AB\" ] = df [ \"AB\" ] . replace ( 0 , np . nan ) df = df . dropna () df [ \"batting_avg\" ] = df [ \"H\" ] / df [ \"AB\" ] df = df [ df [ \"yearID\" ] >= 2016 ] df = df . iloc [ 0 : 15 ] # select out only the first 15 players, just for illustration purposes. df . head ( 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } playerID yearID stint teamID lgID G AB R H 2B ... SB CS BB SO IBB HBP SH SF GIDP batting_avg 101333 abadfe01 2016 1 MIN AL 39 1.0 0 0 0 ... 0.0 0.0 0 1.0 0.0 0.0 0.0 0.0 0.0 0.000000 101335 abreujo02 2016 1 CHA AL 159 624.0 67 183 32 ... 0.0 2.0 47 125.0 7.0 15.0 0.0 9.0 21.0 0.293269 101337 ackledu01 2016 1 NYA AL 28 61.0 6 9 0 ... 0.0 0.0 8 9.0 0.0 0.0 0.0 1.0 0.0 0.147541 101338 adamecr01 2016 1 COL NL 121 225.0 25 49 7 ... 2.0 3.0 24 47.0 0.0 4.0 3.0 0.0 5.0 0.217778 101340 adamsma01 2016 1 SLN NL 118 297.0 37 74 18 ... 0.0 1.0 25 81.0 1.0 2.0 0.0 3.0 5.0 0.249158 5 rows \u00d7 23 columns In this dataset, the columns AB and H are the most relevent. AB is the number of times a player was A t B at. H is the number of times a player h it the ball while batting. The performance of a player can be defined by their batting percentage - essentially the number of hits divided by the number of times at bat. (Technically, a percentage should run from 0-100, but American sportspeople are apparently not very strict with how they approach these definitions.)","title":"Problem Setup"},{"location":"notebooks/hierarchical-baseball/#model-1-naive-model","text":"One model that we can write is a model that assumes that each player has a batting percentage that is independent of the other players in the dataset. A pictorial view of the model is as such: Let's implement this model in PyMC3. with pm . Model () as baseline_model : thetas = pm . Beta ( \"thetas\" , alpha = 0.5 , beta = 0.5 , shape = ( len ( df ))) like = pm . Binomial ( \"likelihood\" , n = df [ \"AB\" ], p = thetas , observed = df [ \"H\" ]) with baseline_model : baseline_trace = pm . sample ( 2000 ) Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [thetas] Sampling 4 chains, 16 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10000/10000 [00:03<00:00, 2887.97draws/s] There were 5 divergences after tuning. Increase `target_accept` or reparameterize. There were 2 divergences after tuning. Increase `target_accept` or reparameterize. There were 8 divergences after tuning. Increase `target_accept` or reparameterize. There was 1 divergence after tuning. Increase `target_accept` or reparameterize. Let's view the posterior distribution traces. import arviz as az traceplot = az . plot_trace ( baseline_trace ) Looks like convergence has been achieved. From a Beta(\\alpha=0.5, \\beta=0.5) Beta(\\alpha=0.5, \\beta=0.5) prior, those players for which we have only 1 data point have very wide posterior distribution estimates. axes = az . plot_forest ( baseline_trace ) One of the big problems that we may have observed above is that the posterior distribution estimates for some players look very \"absurd\", raising a number of questions. For example: Do we really expect the 1 bat, 1 hit player to have such a high estimated batting average? Do we really expect the 1 bat, 0 hits player to have such a low estimated batting average? Don't we usually expect human performance to be approximately symmetrically distributed around some population mean?","title":"Model 1: Naive Model"},{"location":"notebooks/hierarchical-baseball/#model-2-hierarchical-model","text":"With a hierarchical model, we can encode a set of assumptions into the model that may help us address the above questions. By a hierarchical model, we mean that each group's key parameter (in this case, the {\\theta} {\\theta} ) have a parental distribution placed on top of them. This is sometimes anthropomorphically called \"sharing information\" between the distributions. (Not saying that this is a good or bad thing, just naming it as it is.) The model diagram looks something like this: Let's start by specifying the model. with pm . Model () as baseball_model : phi = pm . Uniform ( \"phi\" , lower = 0.0 , upper = 1.0 ) kappa_log = pm . Exponential ( \"kappa_log\" , lam = 1.5 ) kappa = pm . Deterministic ( \"kappa\" , tt . exp ( kappa_log )) thetas = pm . Beta ( \"thetas\" , alpha = phi * kappa , beta = ( 1.0 - phi ) * kappa , shape = len ( df ) ) like = pm . Binomial ( \"like\" , n = df [ \"AB\" ], p = thetas , observed = df [ \"H\" ]) Then we sample from the posterior. with baseball_model : trace = pm . sample ( 2000 ) Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [thetas, kappa_log, phi] Sampling 4 chains, 23 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10000/10000 [00:06<00:00, 1448.88draws/s] There were 23 divergences after tuning. Increase `target_accept` or reparameterize. The number of effective samples is smaller than 10% for some parameters. Finally, let's inspect the traces for convergence. axes = az . plot_trace ( trace ) Convergence looks good! Let's also look at the posterior distribution of batting averages per player. ylabels = \"AB: \" + df [ \"AB\" ] . astype ( str ) + \", H: \" + df [ \"H\" ] . astype ( \"str\" ) ylabels = list ( reversed ( ylabels . tolist ())) axes = az . plot_forest ( trace , var_names = [ \"thetas\" ]) axes [ 0 ] . set_yticklabels ( ylabels ); It appears to me that these estimates are going to be much more reasonable. No player has a wildly high or low estimate on the basis of a single (or very few) data points. With a hierarchical model, we make the assumption that our observations (or treatments that group our observations) are somehow related. Under this assumption, when we have a new sample for which we have very few observations, we are able to borrow power from the population to make inferences about the new sample. Depending on the scenario, this assumption can either be reasonable, thereby not necessitating much debate, or be considered a \"strong assumption\", thereby requiring strong justification.","title":"Model 2: Hierarchical Model"},{"location":"notebooks/hierarchical-baseball/#shrinkage","text":"\"Shrinkage\" is a term used to describe how hierarchical model estimation will usually result in parameter estimates that are \"shrunk\" away from their maximum likelihood estimators (i.e. the naive estimate from the data) towards the global mean. Shrinkage in and of itself is not necessarily a good or bad thing. However, because hierarchical models can sometimes be tricky to get right, we can use a shrinkage plot as a visual diagnostic for whether we have implemented the model correctly. # MLE per player mle = df [ \"batting_avg\" ] . values # Non-hierarchical model no_pool = baseline_trace [ \"thetas\" ] . mean ( axis = 0 ) # Hierarchical model partial_pool = trace [ \"thetas\" ] . mean ( axis = 0 ) # MLE over all players complete_pool = np . array ([ df [ \"batting_avg\" ] . mean ()] * len ( df )) fig = plt . figure () ax = fig . add_subplot ( 111 ) for r in np . vstack ([ mle , no_pool , partial_pool , complete_pool ]) . T : ax . plot ( r ) ax . set_xticks ([ 0 , 1 , 2 , 3 ]) ax . set_xticklabels ( [ \"maximum \\n likelihood\" , \"no \\n pooling\" , \"partial \\n pooling\" , \"complete \\n pooling\" ] ) ax . set_ylim ( 0 , 1 ) (0.0, 1.0) In this shrinkage plot, as we go from the maximum likelihood estimator (MLE) (essentially not explicitly specifying priors) to a no-pooling model (with weak priors), there is \"shrinkage\" of the estimates towards the population mean (complete pooling). Incorporating a parental prior on each group's parameters further constrains the credible range of parameters. This, then, is the phenomenon of \"shrinkage\" in modelling. Just to reiterate again -- there is nothing ineherently right or wrong about shrinkage. Whether this is reasonable or not depends on our prior information about the problem.","title":"Shrinkage"},{"location":"notebooks/markov-models/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); % load_ext autoreload % autoreload 2 % config InlineBackend . figure_format = 'retina' import pymc3 as pm Markov Models From The Bottom Up, with Python Markov models are a useful class of models for sequential-type of data. Before recurrent neural networks (which can be thought of as an upgraded Markov model) came along, Markov Models and their variants were the in thing for processing time series and biological data. Just recently, I was involved in a project with a colleague, Zach Barry, where we thought the use of autoregressive hidden Markov models (AR-HMMs) might be a useful thing. Apart from our hack session one afternoon, it set off a series of self-study that culminated in this essay. By writing this down for my own memory, my hope is that it gives you a resource to refer back to as well. You'll notice that I don't talk about inference (i.e. inferring parameters from data) until the end: this is intentional. As I've learned over the years doing statistical modelling and machine learning, nothing makes sense without first becoming deeply familiar with the \"generative\" story of each model, i.e. the algorithmic steps that let us generate data. It's a very Bayesian-influenced way of thinking that I hope you will become familiar with too. Markov Models: What they are, with mostly plain English and some math The simplest Markov models assume that we have a system that contains a finite set of states, and that the system transitions between these states with some probability at each time step t t , thus generating a sequence of states over time. Let's call these states S S , where S = \\{s_1, s_2, ..., s_n\\} S = \\{s_1, s_2, ..., s_n\\} To keep things simple, let's start with three states: S = \\{s_1, s_2, s_3\\} S = \\{s_1, s_2, s_3\\} A Markov model generates a sequence of states, with one possible realization being: \\{s_1, s_1, s_1, s_3, s_3, s_3, s_2, s_2, s_3, s_3, s_3, s_3, s_1, ...\\} \\{s_1, s_1, s_1, s_3, s_3, s_3, s_2, s_2, s_3, s_3, s_3, s_3, s_1, ...\\} And generically, we represent it as a sequence of states x_t, x_{t+1}... x_{t+n} x_t, x_{t+1}... x_{t+n} . (We have chosen a different symbol to not confuse the \"generic\" state with the specific realization. Graphically, a plain and simple Markov model looks like the following: Initializing a Markov chain Every Markov chain needs to be initialized. To do so, we need an initial state probability vector , which tells us what the distribution of initial states will be. Let's call the vector p_S p_S , where the subscript S S indicates that it is for the \"states\". p_{init} = \\begin{pmatrix} p_1 & p_2 & p_3 \\end{pmatrix} p_{init} = \\begin{pmatrix} p_1 & p_2 & p_3 \\end{pmatrix} Semantically, they allocate the probabilities of starting the sequence at a given state. For example, we might assume a discrete uniform distribution, which in Python would look like: import numpy as np p_init = np . array ([ 1 / 3. , 1 / 3. , 1 / 3. ]) Alternatively, we might assume a fixed starting point, which can be expressed as the p_S p_S array: p_init = np . array ([ 0 , 1 , 0 ]) Alternatively, we might assign non-zero probabilities to each in a non-uniform fashion: # State 0: 0.1 probability # State 1: 0.8 probability # State 2: 0.1 probability p_init = np . array ([ 0.1 , 0.8 , 0.1 ]) Finally, we might assume that the system was long-running before we started observing the sequence of states, and as such the initial state was drawn as one realization of some equilibrated distribution of states. Keep this idea in your head, as we'll need it later. For now, just to keep things concrete, let's specify an initial distribution as a non-uniform probability vector. import numpy as np p_init = np . array ([ 0.1 , 0.8 , 0.1 ]) Modelling transitions between states To know how a system transitions between states, we now need a transition matrix . The transition matrix describes the probability of transitioning from one state to another. (The probability of staying in the same state is semantically equivalent to transitioning to the same state.) By convention, transition matrix rows correspond to the state at time t t , while columns correspond to state at time t+1 t+1 . Hence, row probabilities sum to one, because the probability of transitioning to the next state depends on only the current state, and all possible states are known and enumerated. Let's call the transition matrix P_{transition} P_{transition} . The symbol etymology, which usually gets swept under the rug in mathematically-oriented papers, are as follows: transition transition doesn't refer to time but simply indicates that it is for transitioning states, P P is used because it is a probability matrix. P_{transition} = \\begin{pmatrix} p_{11} & p_{12} & p_{13}\\\\ p_{21} & p_{22} & p_{23}\\\\ p_{31} & p_{32} & p_{33}\\\\ \\end{pmatrix} P_{transition} = \\begin{pmatrix} p_{11} & p_{12} & p_{13}\\\\ p_{21} & p_{22} & p_{23}\\\\ p_{31} & p_{32} & p_{33}\\\\ \\end{pmatrix} Using the transition matrix, we can express that the system likes to stay in the state that it enters into, by assigning larger probability mass to the diagonals. Alternatively, we can express that the system likes to transition out of states that it enters into, by assigning larger probability mass to the off-diagonal. Alrighty, enough with that now, let's initialize a transition matrix below. p_transition = np . array ( [[ 0.90 , 0.05 , 0.05 ], [ 0.01 , 0.90 , 0.09 ], [ 0.07 , 0.03 , 0.9 ]] ) p_transition array([[0.9 , 0.05, 0.05], [0.01, 0.9 , 0.09], [0.07, 0.03, 0.9 ]]) And just to confirm with you that each row sums to one: assert p_transition [ 0 , :] . sum () == 1 assert p_transition [ 1 , :] . sum () == 1 assert p_transition [ 2 , :] . sum () == 1 Equilibrium or Stationary Distribution Now, do you remember how above we talked about the Markov chain being in some \"equilibrated\" state? Well, the stationary or equilibrium distribution of a Markov chain is the distribution of observed states at infinite time. An interesting property is that regardless of what the initial state is, the equilibrium distribution will always be the same, as the equilibrium distribution only depends on the transition matrix. Here's how to think about the equilibrium: if you were to imagine instantiating a thousand Markov chains using the initial distribution p_{init} = \\begin{pmatrix} 0.1 & 0.8 & 0.1 \\end{pmatrix} p_{init} = \\begin{pmatrix} 0.1 & 0.8 & 0.1 \\end{pmatrix} 10% would start out in state 1 80% would start out in state 2 10% would start out in state 3 However, if you ran each of the systems to a large number of time steps (say, 1 million time steps, to exaggerate the point) then how the states were distributed initially wouldn't matter, as how they transition from time step to time step begins to dominate. We could simulate this explicitly in Python, but as it turns out, there is a mathematical shortcut that invovles simple dot products. Let's check it out. Assume we have an initial state and a transition matrix. We're going to reuse p_init from above, but use a different p_transition to make the equilibrium distribution values distinct. This will make it easier for us to plot later. p_transition_example = np . array ( [[ 0.6 , 0.2 , 0.2 ], [ 0.05 , 0.9 , 0.05 ], [ 0.1 , 0.2 , 0.7 ]] ) To simulate the distribution of states in the next time step, we take the initial distribution p_init and matrix multiply it against the transition matrix. p_next = p_init @ p_transition_example p_next array([0.11, 0.76, 0.13]) We can do it again to simulate the distribution of states in the next time step after: p_next = p_next @ p_transition_example p_next array([0.117, 0.732, 0.151]) Let's now write a for-loop to automate the process. p_state_t = [ p_init ] for i in range ( 200 ): # 200 time steps sorta, kinda, approximates infinite time :) p_state_t . append ( p_state_t [ - 1 ] @ p_transition_example ) To make it easier for you to see what we've generated, let's make the p_state_t list into a pandas DataFrame. import pandas as pd state_distributions = pd . DataFrame ( p_state_t ) state_distributions .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 0 0.100000 0.800000 0.10000 1 0.110000 0.760000 0.13000 2 0.117000 0.732000 0.15100 3 0.121900 0.712400 0.16570 4 0.125330 0.698680 0.17599 ... ... ... ... 196 0.133333 0.666667 0.20000 197 0.133333 0.666667 0.20000 198 0.133333 0.666667 0.20000 199 0.133333 0.666667 0.20000 200 0.133333 0.666667 0.20000 201 rows \u00d7 3 columns Now, let's plot what the distributions look like. import matplotlib.pyplot as plt state_distributions . plot (); If you're viewing this notebook on Binder or locally, go ahead and modify the initial state to convince yourself that it doesn't matter what the initial state will be: the equilibrium state distribution, which is the fraction of time the Markov chain is in that state over infinite time, will always be the same as long as the transition matrix stays the same. print ( p_state_t [ - 1 ]) [0.13333333 0.66666667 0.2 ] As it turns out, there's also a way to solve for the equilibrium distribution analytically from the transition matrix. This involves solving a linear algebra problem, which we can do using Python. (Credit goes to this blog post from which I modified the code to fit the variable naming here.) def equilibrium_distribution ( p_transition ): n_states = p_transition . shape [ 0 ] A = np . append ( arr = p_transition . T - np . eye ( n_states ), values = np . ones ( n_states ) . reshape ( 1 , - 1 ), axis = 0 ) b = np . transpose ( np . array ([ 0 ] * n_states + [ 1 ])) p_eq = np . linalg . solve ( a = np . transpose ( A ) . dot ( A ), b = np . transpose ( A ) . dot ( b ) ) return p_eq # alternative def equilibrium_distribution ( p_transition ): \"\"\"This implementation comes from Colin Carroll, who kindly reviewed the notebook\"\"\" n_states = p_transition . shape [ 0 ] A = np . append ( arr = p_transition . T - np . eye ( n_states ), values = np . ones ( n_states ) . reshape ( 1 , - 1 ), axis = 0 ) # Moore-Penrose pseudoinverse = (A^TA)^{-1}A^T pinv = np . linalg . pinv ( A ) # Return last row return pinv . T [ - 1 ] print ( equilibrium_distribution ( p_transition_example )) [0.13333333 0.66666667 0.2 ] Generating a Markov Sequence Generating a Markov sequence means we \"forward\" simulate the chain by: (1) Optionally drawing an initial state from p_S p_S (let's call that s_{t} s_{t} ). This is done by drawing from a multinomial distribution: s_t \\sim Multinomial(1, p_S) s_t \\sim Multinomial(1, p_S) If we assume (and keep in mind that we don't have to) that the system was equilibrated before we started observing its state sequence, then the initial state distribution is equivalent to the equilibrium distribution. All this means that we don't necessarily have to specify the initial distribution explicitly. (2) Drawing the next state by indexing into the transition matrix p_T p_T , and drawing a new state based on the Multinomial distribution: s_{t+1} \\sim Multinomial(1, p_{T_i}) s_{t+1} \\sim Multinomial(1, p_{T_i}) where i i is the index of the state. I previously wrote about what probability distributions are , leveraging the SciPy probability distributions library. We're going to use that extensively here, as opposed to NumPy's random module, so that we can practice getting familiar with probability distributions as objects. In Python code: from scipy.stats import multinomial from typing import List def markov_sequence ( p_init : np . array , p_transition : np . array , sequence_length : int ) -> List [ int ]: \"\"\" Generate a Markov sequence based on p_init and p_transition. \"\"\" if p_init is None : p_init = equilibrium_distribution ( p_transition ) initial_state = list ( multinomial . rvs ( 1 , p_init )) . index ( 1 ) states = [ initial_state ] for _ in range ( sequence_length - 1 ): p_tr = p_transition [ states [ - 1 ]] new_state = list ( multinomial . rvs ( 1 , p_tr )) . index ( 1 ) states . append ( new_state ) return states With this function in hand, let's generate a sequence of length 1000. import seaborn as sns states = markov_sequence ( p_init , p_transition , sequence_length = 1000 ) fig , ax = plt . subplots ( figsize = ( 12 , 4 )) plt . plot ( states ) plt . xlabel ( \"time step\" ) plt . ylabel ( \"state\" ) plt . yticks ([ 0 , 1 , 2 ]) sns . despine () As is pretty evident from the transition probabilities, once this Markov chain enters a state, it tends to maintain its current state rather than transitioning between states. If you've opened up this notebook in Binder or locally, feel free to modify the transition probabilities and initial state probabilities above to see how the Markov sequence changes. If a \"Markov sequence\" feels abstract at this point, one example to help you anchor your understanding would be human motion. The three states can be \"stationary\", \"walking\", and \"running\". We transition between the three states with some probability throughout the day, moving from \"stationary\" (sitting at my desk) to \"walking\" (to get water) to \"stationary\" (because I'm pouring water), to \"walking\" (out the door) to finally \"running\" (for exercise). Emissions: When Markov chains not only produce \"states\", but also observable data So as you've seen above, a Markov chain can produce \"states\". If we are given direct access to the \"states\", then a problem that we may have is inferring the transition probabilities given the states. A more common scenario, however, is that the states are latent , i.e. we cannot directly observe them. Instead, the latent states generate data that are given by some distribution conditioned on the state. We call these Hidden Markov Models . That all sounds abstract, so let's try to make it more concrete. Gaussian Emissions: When Markov chains emit Gaussian-distributed data. With a three state model, we might say that the emissions are Gaussian distributed, but the location ( \\mu \\mu ) and scale ( \\sigma \\sigma ) vary based on which state we are in. In the simplest case: State 1 gives us data y_1 \\sim N(\\mu=1, \\sigma=0.2) y_1 \\sim N(\\mu=1, \\sigma=0.2) State 2 gives us data y_2 \\sim N(\\mu=0, \\sigma=0.5) y_2 \\sim N(\\mu=0, \\sigma=0.5) State 3 gives us data y_3 \\sim N(\\mu=-1, \\sigma=0.1) y_3 \\sim N(\\mu=-1, \\sigma=0.1) In terms of a graphical model, it would look something like this: Turns out, we can model this in Python code too! from scipy.stats import norm def gaussian_emissions ( states : List [ int ], mus : List [ float ], sigmas : List [ float ]) -> List [ float ]: emissions = [] for state in states : loc = mus [ state ] scale = sigmas [ state ] e = norm . rvs ( loc = loc , scale = scale ) emissions . append ( e ) return emissions Let's see what the emissions look like. gaussian_ems = gaussian_emissions ( states , mus = [ 1 , 0 , - 1 ], sigmas = [ 0.2 , 0.5 , 0.1 ]) def plot_emissions ( states , emissions ): fig , axes = plt . subplots ( figsize = ( 16 , 8 ), nrows = 2 , ncols = 1 , sharex = True ) axes [ 0 ] . plot ( states ) axes [ 0 ] . set_title ( \"States\" ) axes [ 1 ] . plot ( emissions ) axes [ 1 ] . set_title ( \"Emissions\" ) sns . despine (); plot_emissions ( states , gaussian_ems ) Emission Distributions can be any valid distribution! Nobody said we have to use Gaussian distributions for emissions; we can, in fact, have a ton of fun and start simulating data using other distributions! Let's try Poisson emissions. Here, then, the poisson rate \\lambda \\lambda is given one per state. In our example below: State 1 gives us data y_1 \\sim Pois(\\lambda=1) y_1 \\sim Pois(\\lambda=1) State 2 gives us data y_2 \\sim Pois(\\lambda=10) y_2 \\sim Pois(\\lambda=10) State 3 gives us data y_3 \\sim Pois(\\lambda=50) y_3 \\sim Pois(\\lambda=50) from scipy.stats import poisson def poisson_emissions ( states : List [ int ], lam : List [ float ]) -> List [ int ]: emissions = [] for state in states : rate = lam [ state ] e = poisson . rvs ( rate ) emissions . append ( e ) return emissions Once again, let's observe the emissions: poisson_ems = poisson_emissions ( states , lam = [ 1 , 10 , 50 ]) plot_emissions ( states , poisson_ems ) Hope the point is made: Take your favourite distribution and use it as the emission distribution, as long as it can serve as a useful model for the data that you observe! Autoregressive Emissions Autoregressive emissions make things even more interesting and flexible! They show up, for example, when we're trying to model \"motion states\" of people or animals: that's because people and animals don't abruptly change from one state to another, but gradually transition in. The \"autoregressive\" component thus helps us model that the emission value does not only depend on the current state, but also on previous state(s), which is what motion data, for example, might look like. How, though, can we enforce this dependency structure? Well, as implied by the term \"structure\", it means we have some set of equations that relate the parameters of the emission distribution to the value of the previous emission. In terms of a generic graphical model, it is represented as follows: Heteroskedastic Autoregressive Emissions Here's a \"simple complex\" example, where the location \\mu_t \\mu_t of the emission distribution at time t t depends on y_{t-1} y_{t-1} , and the scale \\sigma \\sigma depends only on the current state s_t s_t . A place where this model might be useful is when we believe that noise is the only thing that depends on state, while the location follows a random walk. (Stock markets might be an applicable place for this.) In probabilistic notation: y_t \\sim N(\\mu=k y_{t-1}, \\sigma=\\sigma_{s_t}) y_t \\sim N(\\mu=k y_{t-1}, \\sigma=\\sigma_{s_t}) Here, k k is a multiplicative autoregressive coefficient that scales how the previous emission affects the location \\mu \\mu of the current emission. We might also assume that the initial location \\mu=0 \\mu=0 . Because the scale \\sigma \\sigma varies with state, the emissions are called heteroskedastic , which means \"of non-constant variance\". In the example below: State 1 gives us \\sigma=0.5 \\sigma=0.5 (kind of small variance). State 2 gives us \\sigma=0.1 \\sigma=0.1 (smaller variance). State 3 gives us \\sigma=0.01 \\sigma=0.01 (very small varaince). In Python code, we would model it this way: def ar_gaussian_heteroskedastic_emissions ( states : List [ int ], k : float , sigmas : List [ float ]) -> List [ float ]: emissions = [] prev_loc = 0 for state in states : e = norm . rvs ( loc = k * prev_loc , scale = sigmas [ state ]) emissions . append ( e ) prev_loc = e return emissions ar_het_ems = ar_gaussian_heteroskedastic_emissions ( states , k = 1 , sigmas = [ 0.5 , 0.1 , 0.01 ]) plot_emissions ( states , ar_het_ems ) Keep in mind, here, that given the way that we've defined the autoregressive heteroskedastic Gaussian HMM , it is the variance around the heteroskedastic autoregressive emissions that gives us information about the state, not the location. (To see this, notice how every time the system enters into state 2, the chain stops bouncing around much.) Contrast that against vanilla Gaussian emissions that are non-autoregressive: plot_emissions ( states , gaussian_ems ) How does the autoregressive coefficient k k affect the Markov chain emissions? As should be visible, the structure of autoregressiveness can really change how things look! What happens as k k changes? ar_het_ems = ar_gaussian_heteroskedastic_emissions ( states , k = 1 , sigmas = [ 0.5 , 0.1 , 0.01 ]) plot_emissions ( states , ar_het_ems ) ar_het_ems = ar_gaussian_heteroskedastic_emissions ( states , k = 0 , sigmas = [ 0.5 , 0.1 , 0.01 ]) plot_emissions ( states , ar_het_ems ) Interesting stuff! As k \\rightarrow 0 k \\rightarrow 0 , we approach a Gaussian centered exactly on zero, where only the variance of the observations, rather than the collective average location of the observations, give us information about the state. Homoskedastic Autoregressive Emissions What if we wanted instead the variance to remain the same, but desired instead that the emission location \\mu \\mu gives us information about the state while still being autoregressive? Well, we can bake that into the equation structure! y_t \\sim N(\\mu=k y_{t-1} + \\mu_{s_t}, \\sigma=1) y_t \\sim N(\\mu=k y_{t-1} + \\mu_{s_t}, \\sigma=1) In Python code: def ar_gaussian_homoskedastic_emissions ( states : List [ int ], k : float , mus : List [ float ]) -> List [ float ]: emissions = [] prev_loc = 0 for state in states : e = norm . rvs ( loc = k * prev_loc + mus [ state ], scale = 1 ) emissions . append ( e ) prev_loc = e return emissions ar_hom_ems = ar_gaussian_homoskedastic_emissions ( states , k = 1 , mus = [ - 10 , 0 , 10 ]) plot_emissions ( states , ar_hom_ems ) The variance is too small relative to the scale of the data, so it looks like smooth lines. If we change k k , however, we get interesting effects. ar_hom_ems = ar_gaussian_homoskedastic_emissions ( states , k = 0.8 , mus = [ - 10 , 0 , 10 ]) plot_emissions ( states , ar_hom_ems ) Notice how we get \"smoother\" transitions into each state. It's less jumpy. As mentioned earlier, this is extremely useful for modelling motion activity, for example, where people move into and out of states without having jumpy-switching. (We don't go from sitting to standing to walking by jumping frames, we ease into each.) Non-Autoregressive Homoskedastic Emissions With non-autoregressive homoskedastic Gaussian emissions, the mean \\mu \\mu depends only on the hidden state at time t t , and not on the previous hidden state or the previous emission value. In equations: y_t \\sim N(\\mu=f(x_t), \\sigma) y_t \\sim N(\\mu=f(x_t), \\sigma) , where f(x_t) f(x_t) could be a simple mapping: If x_t = 1 x_t = 1 , \\mu = -10 \\mu = -10 , If x_t = 2 x_t = 2 , \\mu = 0 \\mu = 0 , If x_t = 3 x_t = 3 , \\mu = 10 \\mu = 10 . What we can see here is that the mean gives us information about the state, but the scale doesn't. def gaussian_homoskedastic_emissions ( states : List [ int ], mus : List [ float ]) -> List [ float ]: emissions = [] prev_loc = 0 for state in states : e = norm . rvs ( loc = mus [ state ], scale = 1 ) emissions . append ( e ) prev_loc = e return emissions hom_ems = gaussian_homoskedastic_emissions ( states , mus = [ - 10 , 0 , 10 ]) plot_emissions ( states , hom_ems ) As you might intuit from looking at the equations, this is nothing more than a special case of the Heteroskedastic Gaussian Emissions example shown much earlier above. The Framework There's the plain old Markov Model , in which we might generate a sequence of states S S , which are generated from some initial distribution and transition matrix. p_S = \\begin{pmatrix} p_1 & p_2 & p_3 \\end{pmatrix} p_S = \\begin{pmatrix} p_1 & p_2 & p_3 \\end{pmatrix} p_T = \\begin{pmatrix} p_{11} & p_{12} & p_{13}\\\\ p_{21} & p_{22} & p_{23}\\\\ p_{31} & p_{32} & p_{33}\\\\ \\end{pmatrix} p_T = \\begin{pmatrix} p_{11} & p_{12} & p_{13}\\\\ p_{21} & p_{22} & p_{23}\\\\ p_{31} & p_{32} & p_{33}\\\\ \\end{pmatrix} S = \\{s_t, s_{t+1}, ... s_{t+n}\\} S = \\{s_t, s_{t+1}, ... s_{t+n}\\} Graphically: Then there's the \"Hidden\" Markov Model , in which we don't observe the states but rather the emissions generated from the states (according to some assumed distribution). Now, there's not only the initial distribution and transition matrix to worry about, but also the distribution of the emissions conditioned on the state. The general case is when we have some distribution e.g., the Gaussian or the Poisson or the Chi-Squared - whichever fits the likelihood of your data best. Usually, we would pick a parametric distribution both because of modelling convenience and because we think it would help us interpret our data. y_t|s_t \\sim Dist(\\theta_{t}) y_t|s_t \\sim Dist(\\theta_{t}) Where \\theta_t \\theta_t refers to the parameters for the generic distribution Dist Dist that are indexed by the state s_t s_t . (Think back to \"state 1 gives me N(-10, 1) N(-10, 1) , while state 2 gives me N(0, 1) N(0, 1) \", etc...) Your distributions probably generally come from the same family (e.g. \"Gaussians\"), or you can go super complicated and generate them from different distributions. Graphically: Here are some special cases of the general framework. Firstly, the parameters of the emission distribution can be held constant (i.e. simple random walks). This is equivalent to when k=1 k=1 and neither \\mu \\mu nor \\sigma \\sigma depend on current state. In this case, we get back the Gaussian random walk, where y_t \\sim N(k y_{t-1}, \\sigma) y_t \\sim N(k y_{t-1}, \\sigma) ! Secondly, the distribution parameters can depend on the solely on the current state. In this case, you get back basic HMMs! If you make the variance of the likelihood distribution vary based on state, you get heteroskedastic HMMs; conversely, if you keep the variance constant, then you have homoskedastic HMMs. Moving on, there's the \"Autoregressive\" Hidden Markov Models , in which the emissions generated from the states have a dependence on the previous states' emissions (and hence, indirectly, on the previous state). Here, we have the ultimate amount of flexibility to model our processes. y_t|s_t \\sim Dist(f(y_{t-1}, \\theta_t)) y_t|s_t \\sim Dist(f(y_{t-1}, \\theta_t)) Graphically: To keep things simple in this essay, we've only considered the case of lag of 1 (which is where the t-1 t-1 comes from). However, arbitrary numbers of time lags are possible too! And, as usual, you can make them homoskedastic or heteroskedastic by simply controlling the variance parameter of the Dist Dist distribution. Bonus point: your data don't necessarily have to be single dimensional; they can be multidimensional too! As long as you write the f(y_{t-1}, \\theta_t) f(y_{t-1}, \\theta_t) in a fashion that handles y y that are multidimensional, you're golden! Moreover, you can also write the function f f to be any function you like. The function f f doesn't have to be a linear function (like we did); it can instead be a neural network if you so choose, thus giving you a natural progression from Markov models to Recurrent Neural Networks. That, however, is out of scope for this essay. Bayesian Inference on Markov Models Now that we've gone through the \"data generating process\" for Markov sequences with emissions, we can re-examine the entire class of models in a Bayesian light. If you've been observing the models that we've been \"forward-simulating\" all this while to generate data, you'll notice that there are a few key parameters that seemed like, \"well, if we changed them, then the data would change, right?\" If that's what you've been thinking, then bingo! You're on the right track. Moreover, you'll notice that I've couched everything in the language of probability distributions. The transition probabilities P(s_t | s_{t-1}) P(s_t | s_{t-1}) are given by a Multinomial distribution. The emissions are given by an arbitrary continuous (or discrete) distribution, depending on what you believe to be the likelihood distribution for the observed data. Given that we're working with probability distributions and data, you probably have been thinking about it already: we need a way to calculate the log-likelihoods of the data that we observe! (Why we use log-likelihoods instead of likelihoods is clarified here .) Markov Chain Log-Likelihood Calculation Let's examine how we would calculate the log likelihood of state data given the parameters. This will lead us to the Markov chain log-likelihood. The likelihood of a given Markov chain states is: the probability of the first state given some assumed initial distribution, times the probability of the second state given the first state, times the probability of the third state given the second state, and so on... until the end. In math notation, given the states S = \\{s_1, s_2, s_3, ..., s_n\\} S = \\{s_1, s_2, s_3, ..., s_n\\} , this becomes: L(S) = P(s_1) P(s_2|s_1) P(s_3|s_2) ... L(S) = P(s_1) P(s_2|s_1) P(s_3|s_2) ... More explicitly, P(s_1) P(s_1) is nothing more than the probability of observing that state s_1 s_1 given an assumed initial (or equilibrium) distribution: s1 = [ 0 , 1 , 0 ] # assume we start in state 1 of {0, 1, 2} p_eq = equilibrium_distribution ( p_transition ) prob_s1 = p_eq [ s1 . index ( 1 )] prob_s1 0.27896995708154565 Then, P(s_2) P(s_2) is nothing more than the probability of observing that state s_2 s_2 given the transition matrix entry for state s_1 s_1 . # assume we enter into state 2 of {0, 1, 2} s2 = [ 0 , 0 , 1 ] transition_entry = p_transition [ s1 . index ( 1 )] prob_s2 = transition_entry [ s2 . index ( 1 )] prob_s2 0.09 Their joint likelihood is given then by prob_s1 times prob_s2 . prob_s1 * prob_s2 0.025107296137339107 And because we operate in log space to avoid underflow, we do joint log-likelihoods instead: np . log ( prob_s1 ) + np . log ( prob_s2 ) -3.6845967923219334 Let's generalize this in a math function. Since P(s_t|s_{t-1}) P(s_t|s_{t-1}) is a multinomial distribution , then if we are given the log-likelihood of \\{s_1, s_2, s_3, ..., s_n\\} \\{s_1, s_2, s_3, ..., s_n\\} , we can calculate the log-likelihood over \\{s_2,... s_n\\} \\{s_2,... s_n\\} , which is given by the sum of the log probabilities: def state_logp ( states , p_transition ): logp = 0 # states are 0, 1, 2, but we model them as [1, 0, 0], [0, 1, 0], [0, 0, 1] states_oh = np . eye ( len ( p_transition )) for curr_state , next_state in zip ( states [: - 1 ], states [ 1 :]): p_tr = p_transition [ curr_state ] logp += multinomial ( n = 1 , p = p_tr ) . logpmf ( states_oh [ next_state ]) return logp state_logp ( states , p_transition ) -418.65677519562405 We will also write a vectorized version of state_logp . def state_logp_vect ( states , p_transition ): states_oh = np . eye ( len ( p_transition )) p_tr = p_transition [ states [: - 1 ]] obs = states_oh [ states [ 1 :]] return np . sum ( multinomial ( n = 1 , p = p_tr ) . logpmf ( obs )) state_logp_vect ( states , p_transition ) -418.6567751956279 Now, there is a problem here: we also need the log likelihood of the first state. Remember that if we don't know what the initial distribution is supposed to be, one possible assumption we can make is that the Markov sequence began by drawing from the equilibrium distribution. Here is where equilibrium distribution calculation from before comes in handy! def initial_logp ( states , p_transition ): initial_state = states [ 0 ] states_oh = np . eye ( len ( p_transition )) eq_p = equilibrium_distribution ( p_transition ) return ( multinomial ( n = 1 , p = eq_p ) . logpmf ( states_oh [ initial_state ] . squeeze ()) ) initial_logp ( states , p_transition ) array(-1.16057901) Taken together, we get the following Markov chain log-likelihood: def markov_state_logp ( states , p_transition ): return ( state_logp_vect ( states , p_transition ) + initial_logp ( states , p_transition ) ) markov_state_logp ( states , p_transition ) -419.81735420804523 Markov Chain with Gaussian Emissions Log-Likelihood Calculation Now that we know how to calculate the log-likelihood for the Markov chain sequence of states, we can move on to the log-likelihood calculation for the emissions. Let's first assume that we have emissions that are non-autoregressive, and have a Gaussian likelihood. For the benefit of those who need it written out explicitly, here's the for-loop version: def gaussian_logp ( states , mus , sigmas , emissions ): logp = 0 for ( emission , state ) in zip ( emissions , states ): logp += norm ( mus [ state ], sigmas [ state ]) . logpdf ( emission ) return logp gaussian_logp ( states , mus = [ 1 , 0 , - 1 ], sigmas = [ 0.2 , 0.5 , 0.1 ], emissions = gaussian_ems ) 250.57996114495296 And we'll also make a vectorized version of it: def gaussian_logp_vect ( states , mus , sigmas , emissions ): mu = mus [ states ] sigma = sigmas [ states ] return np . sum ( norm ( mu , sigma ) . logpdf ( emissions )) gaussian_logp_vect ( states , mus = np . array ([ 1 , 0 , - 1 ]), sigmas = np . array ([ 0.2 , 0.5 , 0.1 ]), emissions = gaussian_ems ) 250.5799611449528 The joint log likelihood of the emissions and states are then given by their summation. def gaussian_emission_hmm_logp ( states , p_transition , mus , sigmas , emissions ): return markov_state_logp ( states , p_transition ) + gaussian_logp_vect ( states , mus , sigmas , emissions ) gaussian_emission_hmm_logp ( states , p_transition , mus = np . array ([ 1 , 0 , - 1 ]), sigmas = np . array ([ 0.2 , 0.5 , 0.1 ]), emissions = gaussian_ems ) -169.23739306309244 If you're in a Binder or local Jupyter session, go ahead and tweak the values of mus and sigmas , and verify for yourself that the current values are the \"maximum likelihood\" values. After all, our Gaussian emission data were generated according to this exact set of parameters! Markov Chain with Autoregressive Gaussian Emissions Log-Likelihood Calculation I hope the pattern is starting to be clear here: since we have Gaussian emissions, we only have to calculate the parameters of the Gaussian to know what the logpdf would be. As an example, I will be using the Gaussian with: State-varying scale Mean that is dependent on the previously emitted value This is the AR-HMM with data generated from the ar_gaussian_heteroskedastic_emissions function. def ar_gaussian_heteroskedastic_emissions_logp ( states , k , sigmas , emissions ): logp = 0 initial_state = states [ 0 ] initial_emission_logp = norm ( 0 , sigmas [ initial_state ]) . logpdf ( emissions [ 0 ]) for previous_emission , current_emission , state in zip ( emissions [: - 1 ], emissions [ 1 :], states [ 1 :]): loc = k * previous_emission scale = sigmas [ state ] logp += norm ( loc , scale ) . logpdf ( current_emission ) return logp ar_gaussian_heteroskedastic_emissions_logp ( states , k = 1.0 , sigmas = [ 0.5 , 0.1 , 0.01 ], emissions = ar_het_ems ) -18605.714303907385 Now, we can write the full log likelihood of the entire AR-HMM: def ar_gausian_heteroskedastic_hmm_logp ( states , p_transition , k , sigmas , emissions ): return ( markov_state_logp ( states , p_transition ) + ar_gaussian_heteroskedastic_emissions_logp ( states , k , sigmas , emissions ) ) ar_gausian_heteroskedastic_hmm_logp ( states , p_transition , k = 1.0 , sigmas = [ 0.5 , 0.1 , 0.01 ], emissions = ar_het_ems ) -19025.53165811543 For those of you who are familiar with Bayesian inference, as soon as we have a joint log likelihood that we can calculate between our model priors and data, using the simple Bayes' rule equation, we can obtain posterior distributions easily through an MCMC sampler. If this looks all foreign to you, then check out my other essay for a first look (or a refresher)! HMM Distributions in PyMC3 While PyMC4 is in development, PyMC3 remains one of the leading probabilistic programming languages that can be used for Bayesian inference. PyMC3 doesn't have the HMM distribution defined in the library, but thanks to GitHub user @hstrey posting a Jupyter notebook with HMMs defined in there , many PyMC3 users have had a great baseline distribution to study pedagogically and use in their applications, myself included. Side note: I used @hstrey's implementation before setting out to write this essay. Thanks! The key thing to notice in this section is how the logp functions are defined . They will match the log probability functions that we have defined above, except written in Theano. HMM States Distribution Let's first look at the HMM States distribution, which will give us a way to calculate the log probability of the states. import pymc3 as pm import theano.tensor as tt import theano.tensor.slinalg as sla # theano-wrapped scipy linear algebra import theano.tensor.nlinalg as nla # theano-wrapped numpy linear algebra import theano theano . config . gcc . cxxflags = \"-Wno-c++11-narrowing\" class HMMStates ( pm . Categorical ): def __init__ ( self , p_transition , p_equilibrium , n_states , * args , ** kwargs ): \"\"\"You can ignore this section for the time being.\"\"\" super ( pm . Categorical , self ) . __init__ ( * args , ** kwargs ) self . p_transition = p_transition self . p_equilibrium = p_equilibrium # This is needed self . k = n_states # This is only needed because discrete distributions must define a mode. self . mode = tt . cast ( 0 , dtype = 'int64' ) def logp ( self , x ): \"\"\"Focus your attention here!\"\"\" p_eq = self . p_equilibrium # Broadcast out the transition probabilities, # so that we can broadcast the calculation # of log-likelihoods p_tr = self . p_transition [ x [: - 1 ]] # the logp of the initial state evaluated against the equilibrium probabilities initial_state_logp = pm . Categorical . dist ( p_eq ) . logp ( x [ 0 ]) # the logp of the rest of the states. x_i = x [ 1 :] ou_like = pm . Categorical . dist ( p_tr ) . logp ( x_i ) transition_logp = tt . sum ( ou_like ) return initial_state_logp + transition_logp Above, the categorical distribution is used for convenience - it can handle integers, while multinomial requires the one-hot transformation. The categorical distribution is the generalization of the multinomial distribution, but unfortunately, it isn't implemented in the SciPy stats library, which is why we used the multinomial earlier on. Now, we stated earlier on that the transition matrix can be treated as a parameter to tweak, or else a random variable for which we want to infer its parameters. This means there is a natural fit for placing priors on them! Dirichlet distributions are great priors for probability vectors, as they are the generalization of Beta distributions. def solve_equilibrium ( n_states , p_transition ): A = tt . dmatrix ( 'A' ) A = tt . eye ( n_states ) - p_transition + tt . ones ( shape = ( n_states , n_states )) p_equilibrium = pm . Deterministic ( \"p_equilibrium\" , sla . solve ( A . T , tt . ones ( shape = ( n_states )))) return p_equilibrium import warnings warnings . simplefilter ( action = \"ignore\" , category = FutureWarning ) n_states = 3 with pm . Model () as model : p_transition = pm . Dirichlet ( \"p_transition\" , a = tt . ones (( n_states , n_states )) * 4 , # weakly informative prior shape = ( n_states , n_states )) # Solve for the equilibrium state p_equilibrium = solve_equilibrium ( n_states , p_transition ) obs_states = HMMStates ( \"states\" , p_transition = p_transition , p_equilibrium = p_equilibrium , n_states = n_states , observed = np . array ( states ) . astype ( \"float\" ) ) Now let's fit the model! with model : trace = pm . sample ( 2000 ) Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [p_transition] Sampling 4 chains, 0 divergences: 0%| | 0/10000 [00:00<?, ?draws/s]/home/ericmjl/anaconda/envs/bayesian-analysis-recipes/lib/python3.8/site-packages/theano/tensor/slinalg.py:255: LinAlgWarning: Ill-conditioned matrix (rcond=5.89311e-08): result may not be accurate. rval = scipy.linalg.solve(A, b) /home/ericmjl/anaconda/envs/bayesian-analysis-recipes/lib/python3.8/site-packages/theano/tensor/slinalg.py:255: LinAlgWarning: Ill-conditioned matrix (rcond=5.89311e-08): result may not be accurate. rval = scipy.linalg.solve(A, b) Sampling 4 chains, 0 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10000/10000 [00:08<00:00, 1192.11draws/s] import arviz as az az . plot_forest ( trace , var_names = [ \"p_transition\" ]); It looks like we were able to recover the original transitions! HMM with Gaussian Emissions Let's try out now an HMM model with Gaussian emissions. class HMMGaussianEmissions ( pm . Continuous ): def __init__ ( self , states , mu , sigma , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . states = states # self.rate = rate self . mu = mu self . sigma = sigma def logp ( self , x ): \"\"\" x: observations \"\"\" states = self . states # rate = self.rate[states] # broadcast the rate across the states. mu = self . mu [ states ] sigma = self . sigma [ states ] return tt . sum ( pm . Normal . dist ( mu = mu , sigma = sigma ) . logp ( x )) n_states = 3 with pm . Model () as model : # Priors for transition matrix p_transition = pm . Dirichlet ( \"p_transition\" , a = tt . ones (( n_states , n_states )), shape = ( n_states , n_states )) # Solve for the equilibrium state p_equilibrium = solve_equilibrium ( n_states , p_transition ) # HMM state hmm_states = HMMStates ( \"hmm_states\" , p_transition = p_transition , p_equilibrium = p_equilibrium , n_states = n_states , shape = ( len ( gaussian_ems ),) ) # Prior for mu and sigma mu = pm . Normal ( \"mu\" , mu = 0 , sigma = 1 , shape = ( n_states ,)) sigma = pm . Exponential ( \"sigma\" , lam = 2 , shape = ( n_states ,)) # Observed emission likelihood obs = HMMGaussianEmissions ( \"emission\" , states = hmm_states , mu = mu , sigma = sigma , observed = gaussian_ems ) with model : trace = pm . sample ( 2000 ) Multiprocess sampling (4 chains in 4 jobs) CompoundStep >NUTS: [sigma, mu, p_transition] >CategoricalGibbsMetropolis: [hmm_states] Sampling 4 chains, 0 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10000/10000 [11:59<00:00, 13.90draws/s] The rhat statistic is larger than 1.4 for some parameters. The sampler did not converge. The estimated number of effective samples is smaller than 200 for some parameters. az . plot_trace ( trace , var_names = [ \"mu\" ]); az . plot_trace ( trace , var_names = [ \"sigma\" ]); az . plot_forest ( trace , var_names = [ \"sigma\" ]); We are able to recover the parameters, but there is significant intra-chain homogeneity. That is fine, though one way to get around this is to explicitly instantiate prior distributions for each of the parameters instead. Autoregressive HMMs with Gaussian Emissions Let's now add in the autoregressive component to it. The data we will use is the ar_het_ems data, which were generated by using a heteroskedastic assumption, with Gaussian emissions whose mean depends on the previous value, while variance depends on state. As a reminder of what the data look like: ar_het_ems = ar_gaussian_heteroskedastic_emissions ( states , k = 0.6 , sigmas = [ 0.5 , 0.1 , 0.01 ]) plot_emissions ( states , ar_het_ems ) Let's now define the AR-HMM. class ARHMMGaussianEmissions ( pm . Continuous ): def __init__ ( self , states , k , sigma , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . states = states self . sigma = sigma # variance self . k = k # autoregressive coefficient. def logp ( self , x ): \"\"\" x: observations \"\"\" states = self . states sigma = self . sigma [ states ] k = self . k ar_mean = k * x [: - 1 ] ar_like = tt . sum ( pm . Normal . dist ( mu = ar_mean , sigma = sigma [ 1 :]) . logp ( x [ 1 :])) boundary_like = pm . Normal . dist ( mu = 0 , sigma = sigma [ 0 ]) . logp ( x [ 0 ]) return ar_like + boundary_like n_states = 3 with pm . Model () as model : # Priors for transition matrix p_transition = pm . Dirichlet ( \"p_transition\" , a = tt . ones (( n_states , n_states )), shape = ( n_states , n_states )) # Solve for the equilibrium state p_equilibrium = solve_equilibrium ( n_states , p_transition ) # HMM state hmm_states = HMMStates ( \"hmm_states\" , p_transition = p_transition , p_equilibrium = p_equilibrium , n_states = n_states , shape = ( len ( ar_het_ems ),) ) # Prior for sigma and k sigma = pm . Exponential ( \"sigma\" , lam = 2 , shape = ( n_states ,)) k = pm . Beta ( \"k\" , alpha = 2 , beta = 2 ) # a not-so-weak prior for k # Observed emission likelihood obs = ARHMMGaussianEmissions ( \"emission\" , states = hmm_states , sigma = sigma , k = k , observed = ar_het_ems ) with model : trace = pm . sample ( 2000 ) Multiprocess sampling (4 chains in 4 jobs) CompoundStep >NUTS: [k, sigma, p_transition] >CategoricalGibbsMetropolis: [hmm_states] Sampling 4 chains, 6 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10000/10000 [12:34<00:00, 13.26draws/s] The acceptance probability does not match the target. It is 0.9096431867898114, but should be close to 0.8. Try to increase the number of tuning steps. There were 6 divergences after tuning. Increase `target_accept` or reparameterize. The rhat statistic is larger than 1.4 for some parameters. The sampler did not converge. The estimated number of effective samples is smaller than 200 for some parameters. Let's now take a look at the key parameters we might be interested in estimating: k k : the autoregressive coefficient, or how much previous emissions influence current emissions. \\sigma \\sigma : the variance that belongs to each state. az . plot_forest ( trace , var_names = [ \"k\" ]); az . plot_trace ( trace , var_names = [ \"k\" ]); It looks like we were able to obtain the value of k k correctly! az . plot_trace ( trace , var_names = [ \"sigma\" ]); az . plot_forest ( trace , var_names = [ \"sigma\" ]); It also looks like we were able to obtain the correct sigma values too, except that the chains are mixed up. We would do well to take care when calculating means for each parameter on the basis of chains. How about the chain states? Did we get them right? fig , ax = plt . subplots ( figsize = ( 12 , 4 )) plt . plot ( np . round ( trace [ \"hmm_states\" ] . mean ( axis = 0 )), label = \"true\" ) plt . plot ( 2 - np . array ( states ), label = \"inferred\" ) plt . legend (); I had to flip the states because they were backwards relative to the original. Qualitatively, not bad! If we wanted to be a bit more rigorous, we would quantify the accuracy of state identification. If the transition probabilities were a bit more extreme, we might have an easier time with the identifiability of the states. As it stands, because the variance is the only thing that changes, and because the variance of two of the three states are quite similar (one is 0.1 and the other is 0.5), distinguishing between these two states may be more difficult. Concluding Notes Nothing in statistics makes sense... ...unless in light of a \"data generating model\". I initially struggled with the math behind HMMs and its variants, because I had never taken the time to think through the \"data generating process\" carefully. Once we have the data generating process, and in particular, its structure , it becomes trivial to map the structure of the model to the equations that are needed to model it. (I think this is why physicists are such good Bayesians: they are well-trained at thinking about mechanistic, data generating models.) For example, with autoregressive HMMs, until I sat down and thought through the data generating process step-by-step, nothing made sense. Once I wrote out how the mean of the previous observation influenced the mean of the current observation, then things made a ton of sense. In fact, now that I look back on my learning journey in Bayesian statistics, if we can define a likelihood function for our data, we can trivially work backwards and design a data generating process. Model structure is important While writing out the PyMC3 implementations and conditioning them on data, I remember times when I mismatched the model to the data, thus generating posterior samples that exhibited pathologies: divergences and more. This is a reminder that getting the structure of the model is very important. Keep learning I hope this essay was useful for your learning journey as well. If you enjoyed it, please take a moment to star the repository ! Acknowledgements I would like to acknowledge the following colleagues and friends who have helped review the notebook. My colleagues, Zachary Barry and Balaji Goparaju, both of whom pointed out unclear phrasings in my prose and did some code review. Fellow PyMC developers, Colin Carroll (from whom I never cease to learn things), Alex Andorra (who also did code review), Junpeng Lao, Ravin Kumar, and Osvaldo Martin (also for their comments), Professor Allen Downey (of the Olin College of Engineering) who provided important pedagogical comments throughout the notebook.","title":"Markov models"},{"location":"notebooks/markov-models/#markov-models-from-the-bottom-up-with-python","text":"Markov models are a useful class of models for sequential-type of data. Before recurrent neural networks (which can be thought of as an upgraded Markov model) came along, Markov Models and their variants were the in thing for processing time series and biological data. Just recently, I was involved in a project with a colleague, Zach Barry, where we thought the use of autoregressive hidden Markov models (AR-HMMs) might be a useful thing. Apart from our hack session one afternoon, it set off a series of self-study that culminated in this essay. By writing this down for my own memory, my hope is that it gives you a resource to refer back to as well. You'll notice that I don't talk about inference (i.e. inferring parameters from data) until the end: this is intentional. As I've learned over the years doing statistical modelling and machine learning, nothing makes sense without first becoming deeply familiar with the \"generative\" story of each model, i.e. the algorithmic steps that let us generate data. It's a very Bayesian-influenced way of thinking that I hope you will become familiar with too.","title":"Markov Models From The Bottom Up, with Python"},{"location":"notebooks/markov-models/#markov-models-what-they-are-with-mostly-plain-english-and-some-math","text":"The simplest Markov models assume that we have a system that contains a finite set of states, and that the system transitions between these states with some probability at each time step t t , thus generating a sequence of states over time. Let's call these states S S , where S = \\{s_1, s_2, ..., s_n\\} S = \\{s_1, s_2, ..., s_n\\} To keep things simple, let's start with three states: S = \\{s_1, s_2, s_3\\} S = \\{s_1, s_2, s_3\\} A Markov model generates a sequence of states, with one possible realization being: \\{s_1, s_1, s_1, s_3, s_3, s_3, s_2, s_2, s_3, s_3, s_3, s_3, s_1, ...\\} \\{s_1, s_1, s_1, s_3, s_3, s_3, s_2, s_2, s_3, s_3, s_3, s_3, s_1, ...\\} And generically, we represent it as a sequence of states x_t, x_{t+1}... x_{t+n} x_t, x_{t+1}... x_{t+n} . (We have chosen a different symbol to not confuse the \"generic\" state with the specific realization. Graphically, a plain and simple Markov model looks like the following:","title":"Markov Models: What they are, with mostly plain English and some math"},{"location":"notebooks/markov-models/#initializing-a-markov-chain","text":"Every Markov chain needs to be initialized. To do so, we need an initial state probability vector , which tells us what the distribution of initial states will be. Let's call the vector p_S p_S , where the subscript S S indicates that it is for the \"states\". p_{init} = \\begin{pmatrix} p_1 & p_2 & p_3 \\end{pmatrix} p_{init} = \\begin{pmatrix} p_1 & p_2 & p_3 \\end{pmatrix} Semantically, they allocate the probabilities of starting the sequence at a given state. For example, we might assume a discrete uniform distribution, which in Python would look like: import numpy as np p_init = np . array ([ 1 / 3. , 1 / 3. , 1 / 3. ]) Alternatively, we might assume a fixed starting point, which can be expressed as the p_S p_S array: p_init = np . array ([ 0 , 1 , 0 ]) Alternatively, we might assign non-zero probabilities to each in a non-uniform fashion: # State 0: 0.1 probability # State 1: 0.8 probability # State 2: 0.1 probability p_init = np . array ([ 0.1 , 0.8 , 0.1 ]) Finally, we might assume that the system was long-running before we started observing the sequence of states, and as such the initial state was drawn as one realization of some equilibrated distribution of states. Keep this idea in your head, as we'll need it later. For now, just to keep things concrete, let's specify an initial distribution as a non-uniform probability vector. import numpy as np p_init = np . array ([ 0.1 , 0.8 , 0.1 ])","title":"Initializing a Markov chain"},{"location":"notebooks/markov-models/#modelling-transitions-between-states","text":"To know how a system transitions between states, we now need a transition matrix . The transition matrix describes the probability of transitioning from one state to another. (The probability of staying in the same state is semantically equivalent to transitioning to the same state.) By convention, transition matrix rows correspond to the state at time t t , while columns correspond to state at time t+1 t+1 . Hence, row probabilities sum to one, because the probability of transitioning to the next state depends on only the current state, and all possible states are known and enumerated. Let's call the transition matrix P_{transition} P_{transition} . The symbol etymology, which usually gets swept under the rug in mathematically-oriented papers, are as follows: transition transition doesn't refer to time but simply indicates that it is for transitioning states, P P is used because it is a probability matrix. P_{transition} = \\begin{pmatrix} p_{11} & p_{12} & p_{13}\\\\ p_{21} & p_{22} & p_{23}\\\\ p_{31} & p_{32} & p_{33}\\\\ \\end{pmatrix} P_{transition} = \\begin{pmatrix} p_{11} & p_{12} & p_{13}\\\\ p_{21} & p_{22} & p_{23}\\\\ p_{31} & p_{32} & p_{33}\\\\ \\end{pmatrix} Using the transition matrix, we can express that the system likes to stay in the state that it enters into, by assigning larger probability mass to the diagonals. Alternatively, we can express that the system likes to transition out of states that it enters into, by assigning larger probability mass to the off-diagonal. Alrighty, enough with that now, let's initialize a transition matrix below. p_transition = np . array ( [[ 0.90 , 0.05 , 0.05 ], [ 0.01 , 0.90 , 0.09 ], [ 0.07 , 0.03 , 0.9 ]] ) p_transition array([[0.9 , 0.05, 0.05], [0.01, 0.9 , 0.09], [0.07, 0.03, 0.9 ]]) And just to confirm with you that each row sums to one: assert p_transition [ 0 , :] . sum () == 1 assert p_transition [ 1 , :] . sum () == 1 assert p_transition [ 2 , :] . sum () == 1","title":"Modelling transitions between states"},{"location":"notebooks/markov-models/#equilibrium-or-stationary-distribution","text":"Now, do you remember how above we talked about the Markov chain being in some \"equilibrated\" state? Well, the stationary or equilibrium distribution of a Markov chain is the distribution of observed states at infinite time. An interesting property is that regardless of what the initial state is, the equilibrium distribution will always be the same, as the equilibrium distribution only depends on the transition matrix. Here's how to think about the equilibrium: if you were to imagine instantiating a thousand Markov chains using the initial distribution p_{init} = \\begin{pmatrix} 0.1 & 0.8 & 0.1 \\end{pmatrix} p_{init} = \\begin{pmatrix} 0.1 & 0.8 & 0.1 \\end{pmatrix} 10% would start out in state 1 80% would start out in state 2 10% would start out in state 3 However, if you ran each of the systems to a large number of time steps (say, 1 million time steps, to exaggerate the point) then how the states were distributed initially wouldn't matter, as how they transition from time step to time step begins to dominate. We could simulate this explicitly in Python, but as it turns out, there is a mathematical shortcut that invovles simple dot products. Let's check it out. Assume we have an initial state and a transition matrix. We're going to reuse p_init from above, but use a different p_transition to make the equilibrium distribution values distinct. This will make it easier for us to plot later. p_transition_example = np . array ( [[ 0.6 , 0.2 , 0.2 ], [ 0.05 , 0.9 , 0.05 ], [ 0.1 , 0.2 , 0.7 ]] ) To simulate the distribution of states in the next time step, we take the initial distribution p_init and matrix multiply it against the transition matrix. p_next = p_init @ p_transition_example p_next array([0.11, 0.76, 0.13]) We can do it again to simulate the distribution of states in the next time step after: p_next = p_next @ p_transition_example p_next array([0.117, 0.732, 0.151]) Let's now write a for-loop to automate the process. p_state_t = [ p_init ] for i in range ( 200 ): # 200 time steps sorta, kinda, approximates infinite time :) p_state_t . append ( p_state_t [ - 1 ] @ p_transition_example ) To make it easier for you to see what we've generated, let's make the p_state_t list into a pandas DataFrame. import pandas as pd state_distributions = pd . DataFrame ( p_state_t ) state_distributions .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 0 0.100000 0.800000 0.10000 1 0.110000 0.760000 0.13000 2 0.117000 0.732000 0.15100 3 0.121900 0.712400 0.16570 4 0.125330 0.698680 0.17599 ... ... ... ... 196 0.133333 0.666667 0.20000 197 0.133333 0.666667 0.20000 198 0.133333 0.666667 0.20000 199 0.133333 0.666667 0.20000 200 0.133333 0.666667 0.20000 201 rows \u00d7 3 columns Now, let's plot what the distributions look like. import matplotlib.pyplot as plt state_distributions . plot (); If you're viewing this notebook on Binder or locally, go ahead and modify the initial state to convince yourself that it doesn't matter what the initial state will be: the equilibrium state distribution, which is the fraction of time the Markov chain is in that state over infinite time, will always be the same as long as the transition matrix stays the same. print ( p_state_t [ - 1 ]) [0.13333333 0.66666667 0.2 ] As it turns out, there's also a way to solve for the equilibrium distribution analytically from the transition matrix. This involves solving a linear algebra problem, which we can do using Python. (Credit goes to this blog post from which I modified the code to fit the variable naming here.) def equilibrium_distribution ( p_transition ): n_states = p_transition . shape [ 0 ] A = np . append ( arr = p_transition . T - np . eye ( n_states ), values = np . ones ( n_states ) . reshape ( 1 , - 1 ), axis = 0 ) b = np . transpose ( np . array ([ 0 ] * n_states + [ 1 ])) p_eq = np . linalg . solve ( a = np . transpose ( A ) . dot ( A ), b = np . transpose ( A ) . dot ( b ) ) return p_eq # alternative def equilibrium_distribution ( p_transition ): \"\"\"This implementation comes from Colin Carroll, who kindly reviewed the notebook\"\"\" n_states = p_transition . shape [ 0 ] A = np . append ( arr = p_transition . T - np . eye ( n_states ), values = np . ones ( n_states ) . reshape ( 1 , - 1 ), axis = 0 ) # Moore-Penrose pseudoinverse = (A^TA)^{-1}A^T pinv = np . linalg . pinv ( A ) # Return last row return pinv . T [ - 1 ] print ( equilibrium_distribution ( p_transition_example )) [0.13333333 0.66666667 0.2 ]","title":"Equilibrium or Stationary Distribution"},{"location":"notebooks/markov-models/#generating-a-markov-sequence","text":"Generating a Markov sequence means we \"forward\" simulate the chain by: (1) Optionally drawing an initial state from p_S p_S (let's call that s_{t} s_{t} ). This is done by drawing from a multinomial distribution: s_t \\sim Multinomial(1, p_S) s_t \\sim Multinomial(1, p_S) If we assume (and keep in mind that we don't have to) that the system was equilibrated before we started observing its state sequence, then the initial state distribution is equivalent to the equilibrium distribution. All this means that we don't necessarily have to specify the initial distribution explicitly. (2) Drawing the next state by indexing into the transition matrix p_T p_T , and drawing a new state based on the Multinomial distribution: s_{t+1} \\sim Multinomial(1, p_{T_i}) s_{t+1} \\sim Multinomial(1, p_{T_i}) where i i is the index of the state. I previously wrote about what probability distributions are , leveraging the SciPy probability distributions library. We're going to use that extensively here, as opposed to NumPy's random module, so that we can practice getting familiar with probability distributions as objects. In Python code: from scipy.stats import multinomial from typing import List def markov_sequence ( p_init : np . array , p_transition : np . array , sequence_length : int ) -> List [ int ]: \"\"\" Generate a Markov sequence based on p_init and p_transition. \"\"\" if p_init is None : p_init = equilibrium_distribution ( p_transition ) initial_state = list ( multinomial . rvs ( 1 , p_init )) . index ( 1 ) states = [ initial_state ] for _ in range ( sequence_length - 1 ): p_tr = p_transition [ states [ - 1 ]] new_state = list ( multinomial . rvs ( 1 , p_tr )) . index ( 1 ) states . append ( new_state ) return states With this function in hand, let's generate a sequence of length 1000. import seaborn as sns states = markov_sequence ( p_init , p_transition , sequence_length = 1000 ) fig , ax = plt . subplots ( figsize = ( 12 , 4 )) plt . plot ( states ) plt . xlabel ( \"time step\" ) plt . ylabel ( \"state\" ) plt . yticks ([ 0 , 1 , 2 ]) sns . despine () As is pretty evident from the transition probabilities, once this Markov chain enters a state, it tends to maintain its current state rather than transitioning between states. If you've opened up this notebook in Binder or locally, feel free to modify the transition probabilities and initial state probabilities above to see how the Markov sequence changes. If a \"Markov sequence\" feels abstract at this point, one example to help you anchor your understanding would be human motion. The three states can be \"stationary\", \"walking\", and \"running\". We transition between the three states with some probability throughout the day, moving from \"stationary\" (sitting at my desk) to \"walking\" (to get water) to \"stationary\" (because I'm pouring water), to \"walking\" (out the door) to finally \"running\" (for exercise).","title":"Generating a Markov Sequence"},{"location":"notebooks/markov-models/#emissions-when-markov-chains-not-only-produce-states-but-also-observable-data","text":"So as you've seen above, a Markov chain can produce \"states\". If we are given direct access to the \"states\", then a problem that we may have is inferring the transition probabilities given the states. A more common scenario, however, is that the states are latent , i.e. we cannot directly observe them. Instead, the latent states generate data that are given by some distribution conditioned on the state. We call these Hidden Markov Models . That all sounds abstract, so let's try to make it more concrete.","title":"Emissions: When Markov chains not only produce \"states\", but also observable data"},{"location":"notebooks/markov-models/#gaussian-emissions-when-markov-chains-emit-gaussian-distributed-data","text":"With a three state model, we might say that the emissions are Gaussian distributed, but the location ( \\mu \\mu ) and scale ( \\sigma \\sigma ) vary based on which state we are in. In the simplest case: State 1 gives us data y_1 \\sim N(\\mu=1, \\sigma=0.2) y_1 \\sim N(\\mu=1, \\sigma=0.2) State 2 gives us data y_2 \\sim N(\\mu=0, \\sigma=0.5) y_2 \\sim N(\\mu=0, \\sigma=0.5) State 3 gives us data y_3 \\sim N(\\mu=-1, \\sigma=0.1) y_3 \\sim N(\\mu=-1, \\sigma=0.1) In terms of a graphical model, it would look something like this: Turns out, we can model this in Python code too! from scipy.stats import norm def gaussian_emissions ( states : List [ int ], mus : List [ float ], sigmas : List [ float ]) -> List [ float ]: emissions = [] for state in states : loc = mus [ state ] scale = sigmas [ state ] e = norm . rvs ( loc = loc , scale = scale ) emissions . append ( e ) return emissions Let's see what the emissions look like. gaussian_ems = gaussian_emissions ( states , mus = [ 1 , 0 , - 1 ], sigmas = [ 0.2 , 0.5 , 0.1 ]) def plot_emissions ( states , emissions ): fig , axes = plt . subplots ( figsize = ( 16 , 8 ), nrows = 2 , ncols = 1 , sharex = True ) axes [ 0 ] . plot ( states ) axes [ 0 ] . set_title ( \"States\" ) axes [ 1 ] . plot ( emissions ) axes [ 1 ] . set_title ( \"Emissions\" ) sns . despine (); plot_emissions ( states , gaussian_ems )","title":"Gaussian Emissions: When Markov chains emit Gaussian-distributed data."},{"location":"notebooks/markov-models/#emission-distributions-can-be-any-valid-distribution","text":"Nobody said we have to use Gaussian distributions for emissions; we can, in fact, have a ton of fun and start simulating data using other distributions! Let's try Poisson emissions. Here, then, the poisson rate \\lambda \\lambda is given one per state. In our example below: State 1 gives us data y_1 \\sim Pois(\\lambda=1) y_1 \\sim Pois(\\lambda=1) State 2 gives us data y_2 \\sim Pois(\\lambda=10) y_2 \\sim Pois(\\lambda=10) State 3 gives us data y_3 \\sim Pois(\\lambda=50) y_3 \\sim Pois(\\lambda=50) from scipy.stats import poisson def poisson_emissions ( states : List [ int ], lam : List [ float ]) -> List [ int ]: emissions = [] for state in states : rate = lam [ state ] e = poisson . rvs ( rate ) emissions . append ( e ) return emissions Once again, let's observe the emissions: poisson_ems = poisson_emissions ( states , lam = [ 1 , 10 , 50 ]) plot_emissions ( states , poisson_ems ) Hope the point is made: Take your favourite distribution and use it as the emission distribution, as long as it can serve as a useful model for the data that you observe!","title":"Emission Distributions can be any valid distribution!"},{"location":"notebooks/markov-models/#autoregressive-emissions","text":"Autoregressive emissions make things even more interesting and flexible! They show up, for example, when we're trying to model \"motion states\" of people or animals: that's because people and animals don't abruptly change from one state to another, but gradually transition in. The \"autoregressive\" component thus helps us model that the emission value does not only depend on the current state, but also on previous state(s), which is what motion data, for example, might look like. How, though, can we enforce this dependency structure? Well, as implied by the term \"structure\", it means we have some set of equations that relate the parameters of the emission distribution to the value of the previous emission. In terms of a generic graphical model, it is represented as follows:","title":"Autoregressive Emissions"},{"location":"notebooks/markov-models/#heteroskedastic-autoregressive-emissions","text":"Here's a \"simple complex\" example, where the location \\mu_t \\mu_t of the emission distribution at time t t depends on y_{t-1} y_{t-1} , and the scale \\sigma \\sigma depends only on the current state s_t s_t . A place where this model might be useful is when we believe that noise is the only thing that depends on state, while the location follows a random walk. (Stock markets might be an applicable place for this.) In probabilistic notation: y_t \\sim N(\\mu=k y_{t-1}, \\sigma=\\sigma_{s_t}) y_t \\sim N(\\mu=k y_{t-1}, \\sigma=\\sigma_{s_t}) Here, k k is a multiplicative autoregressive coefficient that scales how the previous emission affects the location \\mu \\mu of the current emission. We might also assume that the initial location \\mu=0 \\mu=0 . Because the scale \\sigma \\sigma varies with state, the emissions are called heteroskedastic , which means \"of non-constant variance\". In the example below: State 1 gives us \\sigma=0.5 \\sigma=0.5 (kind of small variance). State 2 gives us \\sigma=0.1 \\sigma=0.1 (smaller variance). State 3 gives us \\sigma=0.01 \\sigma=0.01 (very small varaince). In Python code, we would model it this way: def ar_gaussian_heteroskedastic_emissions ( states : List [ int ], k : float , sigmas : List [ float ]) -> List [ float ]: emissions = [] prev_loc = 0 for state in states : e = norm . rvs ( loc = k * prev_loc , scale = sigmas [ state ]) emissions . append ( e ) prev_loc = e return emissions ar_het_ems = ar_gaussian_heteroskedastic_emissions ( states , k = 1 , sigmas = [ 0.5 , 0.1 , 0.01 ]) plot_emissions ( states , ar_het_ems ) Keep in mind, here, that given the way that we've defined the autoregressive heteroskedastic Gaussian HMM , it is the variance around the heteroskedastic autoregressive emissions that gives us information about the state, not the location. (To see this, notice how every time the system enters into state 2, the chain stops bouncing around much.) Contrast that against vanilla Gaussian emissions that are non-autoregressive: plot_emissions ( states , gaussian_ems )","title":"Heteroskedastic Autoregressive Emissions"},{"location":"notebooks/markov-models/#how-does-the-autoregressive-coefficient-kk-affect-the-markov-chain-emissions","text":"As should be visible, the structure of autoregressiveness can really change how things look! What happens as k k changes? ar_het_ems = ar_gaussian_heteroskedastic_emissions ( states , k = 1 , sigmas = [ 0.5 , 0.1 , 0.01 ]) plot_emissions ( states , ar_het_ems ) ar_het_ems = ar_gaussian_heteroskedastic_emissions ( states , k = 0 , sigmas = [ 0.5 , 0.1 , 0.01 ]) plot_emissions ( states , ar_het_ems ) Interesting stuff! As k \\rightarrow 0 k \\rightarrow 0 , we approach a Gaussian centered exactly on zero, where only the variance of the observations, rather than the collective average location of the observations, give us information about the state.","title":"How does the autoregressive coefficient kk affect the Markov chain emissions?"},{"location":"notebooks/markov-models/#homoskedastic-autoregressive-emissions","text":"What if we wanted instead the variance to remain the same, but desired instead that the emission location \\mu \\mu gives us information about the state while still being autoregressive? Well, we can bake that into the equation structure! y_t \\sim N(\\mu=k y_{t-1} + \\mu_{s_t}, \\sigma=1) y_t \\sim N(\\mu=k y_{t-1} + \\mu_{s_t}, \\sigma=1) In Python code: def ar_gaussian_homoskedastic_emissions ( states : List [ int ], k : float , mus : List [ float ]) -> List [ float ]: emissions = [] prev_loc = 0 for state in states : e = norm . rvs ( loc = k * prev_loc + mus [ state ], scale = 1 ) emissions . append ( e ) prev_loc = e return emissions ar_hom_ems = ar_gaussian_homoskedastic_emissions ( states , k = 1 , mus = [ - 10 , 0 , 10 ]) plot_emissions ( states , ar_hom_ems ) The variance is too small relative to the scale of the data, so it looks like smooth lines. If we change k k , however, we get interesting effects. ar_hom_ems = ar_gaussian_homoskedastic_emissions ( states , k = 0.8 , mus = [ - 10 , 0 , 10 ]) plot_emissions ( states , ar_hom_ems ) Notice how we get \"smoother\" transitions into each state. It's less jumpy. As mentioned earlier, this is extremely useful for modelling motion activity, for example, where people move into and out of states without having jumpy-switching. (We don't go from sitting to standing to walking by jumping frames, we ease into each.)","title":"Homoskedastic Autoregressive Emissions"},{"location":"notebooks/markov-models/#non-autoregressive-homoskedastic-emissions","text":"With non-autoregressive homoskedastic Gaussian emissions, the mean \\mu \\mu depends only on the hidden state at time t t , and not on the previous hidden state or the previous emission value. In equations: y_t \\sim N(\\mu=f(x_t), \\sigma) y_t \\sim N(\\mu=f(x_t), \\sigma) , where f(x_t) f(x_t) could be a simple mapping: If x_t = 1 x_t = 1 , \\mu = -10 \\mu = -10 , If x_t = 2 x_t = 2 , \\mu = 0 \\mu = 0 , If x_t = 3 x_t = 3 , \\mu = 10 \\mu = 10 . What we can see here is that the mean gives us information about the state, but the scale doesn't. def gaussian_homoskedastic_emissions ( states : List [ int ], mus : List [ float ]) -> List [ float ]: emissions = [] prev_loc = 0 for state in states : e = norm . rvs ( loc = mus [ state ], scale = 1 ) emissions . append ( e ) prev_loc = e return emissions hom_ems = gaussian_homoskedastic_emissions ( states , mus = [ - 10 , 0 , 10 ]) plot_emissions ( states , hom_ems ) As you might intuit from looking at the equations, this is nothing more than a special case of the Heteroskedastic Gaussian Emissions example shown much earlier above.","title":"Non-Autoregressive Homoskedastic Emissions"},{"location":"notebooks/markov-models/#the-framework","text":"There's the plain old Markov Model , in which we might generate a sequence of states S S , which are generated from some initial distribution and transition matrix. p_S = \\begin{pmatrix} p_1 & p_2 & p_3 \\end{pmatrix} p_S = \\begin{pmatrix} p_1 & p_2 & p_3 \\end{pmatrix} p_T = \\begin{pmatrix} p_{11} & p_{12} & p_{13}\\\\ p_{21} & p_{22} & p_{23}\\\\ p_{31} & p_{32} & p_{33}\\\\ \\end{pmatrix} p_T = \\begin{pmatrix} p_{11} & p_{12} & p_{13}\\\\ p_{21} & p_{22} & p_{23}\\\\ p_{31} & p_{32} & p_{33}\\\\ \\end{pmatrix} S = \\{s_t, s_{t+1}, ... s_{t+n}\\} S = \\{s_t, s_{t+1}, ... s_{t+n}\\} Graphically: Then there's the \"Hidden\" Markov Model , in which we don't observe the states but rather the emissions generated from the states (according to some assumed distribution). Now, there's not only the initial distribution and transition matrix to worry about, but also the distribution of the emissions conditioned on the state. The general case is when we have some distribution e.g., the Gaussian or the Poisson or the Chi-Squared - whichever fits the likelihood of your data best. Usually, we would pick a parametric distribution both because of modelling convenience and because we think it would help us interpret our data. y_t|s_t \\sim Dist(\\theta_{t}) y_t|s_t \\sim Dist(\\theta_{t}) Where \\theta_t \\theta_t refers to the parameters for the generic distribution Dist Dist that are indexed by the state s_t s_t . (Think back to \"state 1 gives me N(-10, 1) N(-10, 1) , while state 2 gives me N(0, 1) N(0, 1) \", etc...) Your distributions probably generally come from the same family (e.g. \"Gaussians\"), or you can go super complicated and generate them from different distributions. Graphically: Here are some special cases of the general framework. Firstly, the parameters of the emission distribution can be held constant (i.e. simple random walks). This is equivalent to when k=1 k=1 and neither \\mu \\mu nor \\sigma \\sigma depend on current state. In this case, we get back the Gaussian random walk, where y_t \\sim N(k y_{t-1}, \\sigma) y_t \\sim N(k y_{t-1}, \\sigma) ! Secondly, the distribution parameters can depend on the solely on the current state. In this case, you get back basic HMMs! If you make the variance of the likelihood distribution vary based on state, you get heteroskedastic HMMs; conversely, if you keep the variance constant, then you have homoskedastic HMMs. Moving on, there's the \"Autoregressive\" Hidden Markov Models , in which the emissions generated from the states have a dependence on the previous states' emissions (and hence, indirectly, on the previous state). Here, we have the ultimate amount of flexibility to model our processes. y_t|s_t \\sim Dist(f(y_{t-1}, \\theta_t)) y_t|s_t \\sim Dist(f(y_{t-1}, \\theta_t)) Graphically: To keep things simple in this essay, we've only considered the case of lag of 1 (which is where the t-1 t-1 comes from). However, arbitrary numbers of time lags are possible too! And, as usual, you can make them homoskedastic or heteroskedastic by simply controlling the variance parameter of the Dist Dist distribution. Bonus point: your data don't necessarily have to be single dimensional; they can be multidimensional too! As long as you write the f(y_{t-1}, \\theta_t) f(y_{t-1}, \\theta_t) in a fashion that handles y y that are multidimensional, you're golden! Moreover, you can also write the function f f to be any function you like. The function f f doesn't have to be a linear function (like we did); it can instead be a neural network if you so choose, thus giving you a natural progression from Markov models to Recurrent Neural Networks. That, however, is out of scope for this essay.","title":"The Framework"},{"location":"notebooks/markov-models/#bayesian-inference-on-markov-models","text":"Now that we've gone through the \"data generating process\" for Markov sequences with emissions, we can re-examine the entire class of models in a Bayesian light. If you've been observing the models that we've been \"forward-simulating\" all this while to generate data, you'll notice that there are a few key parameters that seemed like, \"well, if we changed them, then the data would change, right?\" If that's what you've been thinking, then bingo! You're on the right track. Moreover, you'll notice that I've couched everything in the language of probability distributions. The transition probabilities P(s_t | s_{t-1}) P(s_t | s_{t-1}) are given by a Multinomial distribution. The emissions are given by an arbitrary continuous (or discrete) distribution, depending on what you believe to be the likelihood distribution for the observed data. Given that we're working with probability distributions and data, you probably have been thinking about it already: we need a way to calculate the log-likelihoods of the data that we observe! (Why we use log-likelihoods instead of likelihoods is clarified here .)","title":"Bayesian Inference on Markov Models"},{"location":"notebooks/markov-models/#markov-chain-log-likelihood-calculation","text":"Let's examine how we would calculate the log likelihood of state data given the parameters. This will lead us to the Markov chain log-likelihood. The likelihood of a given Markov chain states is: the probability of the first state given some assumed initial distribution, times the probability of the second state given the first state, times the probability of the third state given the second state, and so on... until the end. In math notation, given the states S = \\{s_1, s_2, s_3, ..., s_n\\} S = \\{s_1, s_2, s_3, ..., s_n\\} , this becomes: L(S) = P(s_1) P(s_2|s_1) P(s_3|s_2) ... L(S) = P(s_1) P(s_2|s_1) P(s_3|s_2) ... More explicitly, P(s_1) P(s_1) is nothing more than the probability of observing that state s_1 s_1 given an assumed initial (or equilibrium) distribution: s1 = [ 0 , 1 , 0 ] # assume we start in state 1 of {0, 1, 2} p_eq = equilibrium_distribution ( p_transition ) prob_s1 = p_eq [ s1 . index ( 1 )] prob_s1 0.27896995708154565 Then, P(s_2) P(s_2) is nothing more than the probability of observing that state s_2 s_2 given the transition matrix entry for state s_1 s_1 . # assume we enter into state 2 of {0, 1, 2} s2 = [ 0 , 0 , 1 ] transition_entry = p_transition [ s1 . index ( 1 )] prob_s2 = transition_entry [ s2 . index ( 1 )] prob_s2 0.09 Their joint likelihood is given then by prob_s1 times prob_s2 . prob_s1 * prob_s2 0.025107296137339107 And because we operate in log space to avoid underflow, we do joint log-likelihoods instead: np . log ( prob_s1 ) + np . log ( prob_s2 ) -3.6845967923219334 Let's generalize this in a math function. Since P(s_t|s_{t-1}) P(s_t|s_{t-1}) is a multinomial distribution , then if we are given the log-likelihood of \\{s_1, s_2, s_3, ..., s_n\\} \\{s_1, s_2, s_3, ..., s_n\\} , we can calculate the log-likelihood over \\{s_2,... s_n\\} \\{s_2,... s_n\\} , which is given by the sum of the log probabilities: def state_logp ( states , p_transition ): logp = 0 # states are 0, 1, 2, but we model them as [1, 0, 0], [0, 1, 0], [0, 0, 1] states_oh = np . eye ( len ( p_transition )) for curr_state , next_state in zip ( states [: - 1 ], states [ 1 :]): p_tr = p_transition [ curr_state ] logp += multinomial ( n = 1 , p = p_tr ) . logpmf ( states_oh [ next_state ]) return logp state_logp ( states , p_transition ) -418.65677519562405 We will also write a vectorized version of state_logp . def state_logp_vect ( states , p_transition ): states_oh = np . eye ( len ( p_transition )) p_tr = p_transition [ states [: - 1 ]] obs = states_oh [ states [ 1 :]] return np . sum ( multinomial ( n = 1 , p = p_tr ) . logpmf ( obs )) state_logp_vect ( states , p_transition ) -418.6567751956279 Now, there is a problem here: we also need the log likelihood of the first state. Remember that if we don't know what the initial distribution is supposed to be, one possible assumption we can make is that the Markov sequence began by drawing from the equilibrium distribution. Here is where equilibrium distribution calculation from before comes in handy! def initial_logp ( states , p_transition ): initial_state = states [ 0 ] states_oh = np . eye ( len ( p_transition )) eq_p = equilibrium_distribution ( p_transition ) return ( multinomial ( n = 1 , p = eq_p ) . logpmf ( states_oh [ initial_state ] . squeeze ()) ) initial_logp ( states , p_transition ) array(-1.16057901) Taken together, we get the following Markov chain log-likelihood: def markov_state_logp ( states , p_transition ): return ( state_logp_vect ( states , p_transition ) + initial_logp ( states , p_transition ) ) markov_state_logp ( states , p_transition ) -419.81735420804523","title":"Markov Chain Log-Likelihood Calculation"},{"location":"notebooks/markov-models/#markov-chain-with-gaussian-emissions-log-likelihood-calculation","text":"Now that we know how to calculate the log-likelihood for the Markov chain sequence of states, we can move on to the log-likelihood calculation for the emissions. Let's first assume that we have emissions that are non-autoregressive, and have a Gaussian likelihood. For the benefit of those who need it written out explicitly, here's the for-loop version: def gaussian_logp ( states , mus , sigmas , emissions ): logp = 0 for ( emission , state ) in zip ( emissions , states ): logp += norm ( mus [ state ], sigmas [ state ]) . logpdf ( emission ) return logp gaussian_logp ( states , mus = [ 1 , 0 , - 1 ], sigmas = [ 0.2 , 0.5 , 0.1 ], emissions = gaussian_ems ) 250.57996114495296 And we'll also make a vectorized version of it: def gaussian_logp_vect ( states , mus , sigmas , emissions ): mu = mus [ states ] sigma = sigmas [ states ] return np . sum ( norm ( mu , sigma ) . logpdf ( emissions )) gaussian_logp_vect ( states , mus = np . array ([ 1 , 0 , - 1 ]), sigmas = np . array ([ 0.2 , 0.5 , 0.1 ]), emissions = gaussian_ems ) 250.5799611449528 The joint log likelihood of the emissions and states are then given by their summation. def gaussian_emission_hmm_logp ( states , p_transition , mus , sigmas , emissions ): return markov_state_logp ( states , p_transition ) + gaussian_logp_vect ( states , mus , sigmas , emissions ) gaussian_emission_hmm_logp ( states , p_transition , mus = np . array ([ 1 , 0 , - 1 ]), sigmas = np . array ([ 0.2 , 0.5 , 0.1 ]), emissions = gaussian_ems ) -169.23739306309244 If you're in a Binder or local Jupyter session, go ahead and tweak the values of mus and sigmas , and verify for yourself that the current values are the \"maximum likelihood\" values. After all, our Gaussian emission data were generated according to this exact set of parameters!","title":"Markov Chain with Gaussian Emissions Log-Likelihood Calculation"},{"location":"notebooks/markov-models/#markov-chain-with-autoregressive-gaussian-emissions-log-likelihood-calculation","text":"I hope the pattern is starting to be clear here: since we have Gaussian emissions, we only have to calculate the parameters of the Gaussian to know what the logpdf would be. As an example, I will be using the Gaussian with: State-varying scale Mean that is dependent on the previously emitted value This is the AR-HMM with data generated from the ar_gaussian_heteroskedastic_emissions function. def ar_gaussian_heteroskedastic_emissions_logp ( states , k , sigmas , emissions ): logp = 0 initial_state = states [ 0 ] initial_emission_logp = norm ( 0 , sigmas [ initial_state ]) . logpdf ( emissions [ 0 ]) for previous_emission , current_emission , state in zip ( emissions [: - 1 ], emissions [ 1 :], states [ 1 :]): loc = k * previous_emission scale = sigmas [ state ] logp += norm ( loc , scale ) . logpdf ( current_emission ) return logp ar_gaussian_heteroskedastic_emissions_logp ( states , k = 1.0 , sigmas = [ 0.5 , 0.1 , 0.01 ], emissions = ar_het_ems ) -18605.714303907385 Now, we can write the full log likelihood of the entire AR-HMM: def ar_gausian_heteroskedastic_hmm_logp ( states , p_transition , k , sigmas , emissions ): return ( markov_state_logp ( states , p_transition ) + ar_gaussian_heteroskedastic_emissions_logp ( states , k , sigmas , emissions ) ) ar_gausian_heteroskedastic_hmm_logp ( states , p_transition , k = 1.0 , sigmas = [ 0.5 , 0.1 , 0.01 ], emissions = ar_het_ems ) -19025.53165811543 For those of you who are familiar with Bayesian inference, as soon as we have a joint log likelihood that we can calculate between our model priors and data, using the simple Bayes' rule equation, we can obtain posterior distributions easily through an MCMC sampler. If this looks all foreign to you, then check out my other essay for a first look (or a refresher)!","title":"Markov Chain with Autoregressive Gaussian Emissions Log-Likelihood Calculation"},{"location":"notebooks/markov-models/#hmm-distributions-in-pymc3","text":"While PyMC4 is in development, PyMC3 remains one of the leading probabilistic programming languages that can be used for Bayesian inference. PyMC3 doesn't have the HMM distribution defined in the library, but thanks to GitHub user @hstrey posting a Jupyter notebook with HMMs defined in there , many PyMC3 users have had a great baseline distribution to study pedagogically and use in their applications, myself included. Side note: I used @hstrey's implementation before setting out to write this essay. Thanks! The key thing to notice in this section is how the logp functions are defined . They will match the log probability functions that we have defined above, except written in Theano.","title":"HMM Distributions in PyMC3"},{"location":"notebooks/markov-models/#hmm-states-distribution","text":"Let's first look at the HMM States distribution, which will give us a way to calculate the log probability of the states. import pymc3 as pm import theano.tensor as tt import theano.tensor.slinalg as sla # theano-wrapped scipy linear algebra import theano.tensor.nlinalg as nla # theano-wrapped numpy linear algebra import theano theano . config . gcc . cxxflags = \"-Wno-c++11-narrowing\" class HMMStates ( pm . Categorical ): def __init__ ( self , p_transition , p_equilibrium , n_states , * args , ** kwargs ): \"\"\"You can ignore this section for the time being.\"\"\" super ( pm . Categorical , self ) . __init__ ( * args , ** kwargs ) self . p_transition = p_transition self . p_equilibrium = p_equilibrium # This is needed self . k = n_states # This is only needed because discrete distributions must define a mode. self . mode = tt . cast ( 0 , dtype = 'int64' ) def logp ( self , x ): \"\"\"Focus your attention here!\"\"\" p_eq = self . p_equilibrium # Broadcast out the transition probabilities, # so that we can broadcast the calculation # of log-likelihoods p_tr = self . p_transition [ x [: - 1 ]] # the logp of the initial state evaluated against the equilibrium probabilities initial_state_logp = pm . Categorical . dist ( p_eq ) . logp ( x [ 0 ]) # the logp of the rest of the states. x_i = x [ 1 :] ou_like = pm . Categorical . dist ( p_tr ) . logp ( x_i ) transition_logp = tt . sum ( ou_like ) return initial_state_logp + transition_logp Above, the categorical distribution is used for convenience - it can handle integers, while multinomial requires the one-hot transformation. The categorical distribution is the generalization of the multinomial distribution, but unfortunately, it isn't implemented in the SciPy stats library, which is why we used the multinomial earlier on. Now, we stated earlier on that the transition matrix can be treated as a parameter to tweak, or else a random variable for which we want to infer its parameters. This means there is a natural fit for placing priors on them! Dirichlet distributions are great priors for probability vectors, as they are the generalization of Beta distributions. def solve_equilibrium ( n_states , p_transition ): A = tt . dmatrix ( 'A' ) A = tt . eye ( n_states ) - p_transition + tt . ones ( shape = ( n_states , n_states )) p_equilibrium = pm . Deterministic ( \"p_equilibrium\" , sla . solve ( A . T , tt . ones ( shape = ( n_states )))) return p_equilibrium import warnings warnings . simplefilter ( action = \"ignore\" , category = FutureWarning ) n_states = 3 with pm . Model () as model : p_transition = pm . Dirichlet ( \"p_transition\" , a = tt . ones (( n_states , n_states )) * 4 , # weakly informative prior shape = ( n_states , n_states )) # Solve for the equilibrium state p_equilibrium = solve_equilibrium ( n_states , p_transition ) obs_states = HMMStates ( \"states\" , p_transition = p_transition , p_equilibrium = p_equilibrium , n_states = n_states , observed = np . array ( states ) . astype ( \"float\" ) ) Now let's fit the model! with model : trace = pm . sample ( 2000 ) Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [p_transition] Sampling 4 chains, 0 divergences: 0%| | 0/10000 [00:00<?, ?draws/s]/home/ericmjl/anaconda/envs/bayesian-analysis-recipes/lib/python3.8/site-packages/theano/tensor/slinalg.py:255: LinAlgWarning: Ill-conditioned matrix (rcond=5.89311e-08): result may not be accurate. rval = scipy.linalg.solve(A, b) /home/ericmjl/anaconda/envs/bayesian-analysis-recipes/lib/python3.8/site-packages/theano/tensor/slinalg.py:255: LinAlgWarning: Ill-conditioned matrix (rcond=5.89311e-08): result may not be accurate. rval = scipy.linalg.solve(A, b) Sampling 4 chains, 0 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10000/10000 [00:08<00:00, 1192.11draws/s] import arviz as az az . plot_forest ( trace , var_names = [ \"p_transition\" ]); It looks like we were able to recover the original transitions!","title":"HMM States Distribution"},{"location":"notebooks/markov-models/#hmm-with-gaussian-emissions","text":"Let's try out now an HMM model with Gaussian emissions. class HMMGaussianEmissions ( pm . Continuous ): def __init__ ( self , states , mu , sigma , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . states = states # self.rate = rate self . mu = mu self . sigma = sigma def logp ( self , x ): \"\"\" x: observations \"\"\" states = self . states # rate = self.rate[states] # broadcast the rate across the states. mu = self . mu [ states ] sigma = self . sigma [ states ] return tt . sum ( pm . Normal . dist ( mu = mu , sigma = sigma ) . logp ( x )) n_states = 3 with pm . Model () as model : # Priors for transition matrix p_transition = pm . Dirichlet ( \"p_transition\" , a = tt . ones (( n_states , n_states )), shape = ( n_states , n_states )) # Solve for the equilibrium state p_equilibrium = solve_equilibrium ( n_states , p_transition ) # HMM state hmm_states = HMMStates ( \"hmm_states\" , p_transition = p_transition , p_equilibrium = p_equilibrium , n_states = n_states , shape = ( len ( gaussian_ems ),) ) # Prior for mu and sigma mu = pm . Normal ( \"mu\" , mu = 0 , sigma = 1 , shape = ( n_states ,)) sigma = pm . Exponential ( \"sigma\" , lam = 2 , shape = ( n_states ,)) # Observed emission likelihood obs = HMMGaussianEmissions ( \"emission\" , states = hmm_states , mu = mu , sigma = sigma , observed = gaussian_ems ) with model : trace = pm . sample ( 2000 ) Multiprocess sampling (4 chains in 4 jobs) CompoundStep >NUTS: [sigma, mu, p_transition] >CategoricalGibbsMetropolis: [hmm_states] Sampling 4 chains, 0 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10000/10000 [11:59<00:00, 13.90draws/s] The rhat statistic is larger than 1.4 for some parameters. The sampler did not converge. The estimated number of effective samples is smaller than 200 for some parameters. az . plot_trace ( trace , var_names = [ \"mu\" ]); az . plot_trace ( trace , var_names = [ \"sigma\" ]); az . plot_forest ( trace , var_names = [ \"sigma\" ]); We are able to recover the parameters, but there is significant intra-chain homogeneity. That is fine, though one way to get around this is to explicitly instantiate prior distributions for each of the parameters instead.","title":"HMM with Gaussian Emissions"},{"location":"notebooks/markov-models/#autoregressive-hmms-with-gaussian-emissions","text":"Let's now add in the autoregressive component to it. The data we will use is the ar_het_ems data, which were generated by using a heteroskedastic assumption, with Gaussian emissions whose mean depends on the previous value, while variance depends on state. As a reminder of what the data look like: ar_het_ems = ar_gaussian_heteroskedastic_emissions ( states , k = 0.6 , sigmas = [ 0.5 , 0.1 , 0.01 ]) plot_emissions ( states , ar_het_ems ) Let's now define the AR-HMM. class ARHMMGaussianEmissions ( pm . Continuous ): def __init__ ( self , states , k , sigma , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . states = states self . sigma = sigma # variance self . k = k # autoregressive coefficient. def logp ( self , x ): \"\"\" x: observations \"\"\" states = self . states sigma = self . sigma [ states ] k = self . k ar_mean = k * x [: - 1 ] ar_like = tt . sum ( pm . Normal . dist ( mu = ar_mean , sigma = sigma [ 1 :]) . logp ( x [ 1 :])) boundary_like = pm . Normal . dist ( mu = 0 , sigma = sigma [ 0 ]) . logp ( x [ 0 ]) return ar_like + boundary_like n_states = 3 with pm . Model () as model : # Priors for transition matrix p_transition = pm . Dirichlet ( \"p_transition\" , a = tt . ones (( n_states , n_states )), shape = ( n_states , n_states )) # Solve for the equilibrium state p_equilibrium = solve_equilibrium ( n_states , p_transition ) # HMM state hmm_states = HMMStates ( \"hmm_states\" , p_transition = p_transition , p_equilibrium = p_equilibrium , n_states = n_states , shape = ( len ( ar_het_ems ),) ) # Prior for sigma and k sigma = pm . Exponential ( \"sigma\" , lam = 2 , shape = ( n_states ,)) k = pm . Beta ( \"k\" , alpha = 2 , beta = 2 ) # a not-so-weak prior for k # Observed emission likelihood obs = ARHMMGaussianEmissions ( \"emission\" , states = hmm_states , sigma = sigma , k = k , observed = ar_het_ems ) with model : trace = pm . sample ( 2000 ) Multiprocess sampling (4 chains in 4 jobs) CompoundStep >NUTS: [k, sigma, p_transition] >CategoricalGibbsMetropolis: [hmm_states] Sampling 4 chains, 6 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10000/10000 [12:34<00:00, 13.26draws/s] The acceptance probability does not match the target. It is 0.9096431867898114, but should be close to 0.8. Try to increase the number of tuning steps. There were 6 divergences after tuning. Increase `target_accept` or reparameterize. The rhat statistic is larger than 1.4 for some parameters. The sampler did not converge. The estimated number of effective samples is smaller than 200 for some parameters. Let's now take a look at the key parameters we might be interested in estimating: k k : the autoregressive coefficient, or how much previous emissions influence current emissions. \\sigma \\sigma : the variance that belongs to each state. az . plot_forest ( trace , var_names = [ \"k\" ]); az . plot_trace ( trace , var_names = [ \"k\" ]); It looks like we were able to obtain the value of k k correctly! az . plot_trace ( trace , var_names = [ \"sigma\" ]); az . plot_forest ( trace , var_names = [ \"sigma\" ]); It also looks like we were able to obtain the correct sigma values too, except that the chains are mixed up. We would do well to take care when calculating means for each parameter on the basis of chains. How about the chain states? Did we get them right? fig , ax = plt . subplots ( figsize = ( 12 , 4 )) plt . plot ( np . round ( trace [ \"hmm_states\" ] . mean ( axis = 0 )), label = \"true\" ) plt . plot ( 2 - np . array ( states ), label = \"inferred\" ) plt . legend (); I had to flip the states because they were backwards relative to the original. Qualitatively, not bad! If we wanted to be a bit more rigorous, we would quantify the accuracy of state identification. If the transition probabilities were a bit more extreme, we might have an easier time with the identifiability of the states. As it stands, because the variance is the only thing that changes, and because the variance of two of the three states are quite similar (one is 0.1 and the other is 0.5), distinguishing between these two states may be more difficult.","title":"Autoregressive HMMs with Gaussian Emissions"},{"location":"notebooks/markov-models/#concluding-notes","text":"","title":"Concluding Notes"},{"location":"notebooks/markov-models/#nothing-in-statistics-makes-sense","text":"...unless in light of a \"data generating model\". I initially struggled with the math behind HMMs and its variants, because I had never taken the time to think through the \"data generating process\" carefully. Once we have the data generating process, and in particular, its structure , it becomes trivial to map the structure of the model to the equations that are needed to model it. (I think this is why physicists are such good Bayesians: they are well-trained at thinking about mechanistic, data generating models.) For example, with autoregressive HMMs, until I sat down and thought through the data generating process step-by-step, nothing made sense. Once I wrote out how the mean of the previous observation influenced the mean of the current observation, then things made a ton of sense. In fact, now that I look back on my learning journey in Bayesian statistics, if we can define a likelihood function for our data, we can trivially work backwards and design a data generating process.","title":"Nothing in statistics makes sense..."},{"location":"notebooks/markov-models/#model-structure-is-important","text":"While writing out the PyMC3 implementations and conditioning them on data, I remember times when I mismatched the model to the data, thus generating posterior samples that exhibited pathologies: divergences and more. This is a reminder that getting the structure of the model is very important.","title":"Model structure is important"},{"location":"notebooks/markov-models/#keep-learning","text":"I hope this essay was useful for your learning journey as well. If you enjoyed it, please take a moment to star the repository !","title":"Keep learning"},{"location":"notebooks/markov-models/#acknowledgements","text":"I would like to acknowledge the following colleagues and friends who have helped review the notebook. My colleagues, Zachary Barry and Balaji Goparaju, both of whom pointed out unclear phrasings in my prose and did some code review. Fellow PyMC developers, Colin Carroll (from whom I never cease to learn things), Alex Andorra (who also did code review), Junpeng Lao, Ravin Kumar, and Osvaldo Martin (also for their comments), Professor Allen Downey (of the Olin College of Engineering) who provided important pedagogical comments throughout the notebook.","title":"Acknowledgements"}]}